{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> <font color = \"bisque\"> Tareas:</font> </u>\n",
    "## <font color = \"bisque\"> * Implementar procesos de renombre </font> \n",
    "\n",
    "---\n",
    "\n",
    "<b> <font color = \"LightSalmon\"> MODO DE TRABAJO PROCESO DE RENOMBRE - v2 </font> </b>\n",
    "\n",
    "Voy a trabajar sobre argendata/data que esté hasta ahora, en un fork propio.\n",
    "\n",
    "<b> Sobre argendata/data lo que voy a hacer es tomar los .csv  originales y hacer lo siguiente (dependiendo el caso), generando .csv actualizados:</b>\n",
    "\n",
    "\n",
    "   - Si un dataset tiene un código que esté incluido en los geocodigos del geonomenclador, se nomencla con la string de name_long\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que matchee sin ambigüedad en desc_fundar, entonces se usa el código que está en el geonomenclador (y se guarda la asociación del codigo encontrado con el codigo usado)\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que NO matchea sin ambigüedad en desc_fundar, se genera un código nuevo (a revisión para ser desambiguado con posibles matches)\n",
    "\n",
    "   - Si un dataset tiene una string que matchee sin ambigüedad en desc_fundar entonces se usa el código que está en el geonomenclador para esa string\n",
    "\n",
    "   - Si un dataset tiene una string que NO matchee sin ambigüedad en desc_fundar, entonces se genera un código nuevo (a revisión para ser desambiguado con posibles matches)\n",
    "\n",
    "\n",
    "<b> ¿Cómo busco las columnas de interés para esta iteración? </b>\n",
    "- X% (por ejemplo 80%) del contenido de columnas de interés de un .csv de un tópico es parte de alguna de las columnas del geoneomenclador \n",
    "- Por el momento solo código que esté en el geonomenclador en español\n",
    "\n",
    "Observaciones: \n",
    "- PARA ESTE LABURO ARMÉ MI PROPIO BLOQUE DE ENCODERs, DELIMITERs\n",
    "- Para esta versión de la tarea, en principio, solo consideraría contenido en español\n",
    "\n",
    "<b> <font color = \"LightSalmon\"> Productos: \n",
    "- .csv argendata con agregado de columnas para implementar renombres correspondientes\n",
    "- .csv con lo que mencioné antes con .csv de argendata que presentarían problemas </font> </b>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <u> <font color = \"orangered\"> Cómo abrir cada .csv considerando su encoding: </font> </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"gold\"> Cada .csv de Argendata tiene su propio encodings y delimiters </font>. \n",
    "\n",
    "No todos los .csv que contempla Argendata tienen el mismo enconding y delimiter. Por eso antes de comenzar a trabajar voy a armar el siguiente input para la tarea: dataframe que contenga 4 columnas que sean 'TOPICO', 'archivo_csv', 'encoding' y delimiter'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TOPICO                     archivo_csv    encoding delimiter\n",
      "0    SEBACO              01_ventas_afip.csv       ascii         ,\n",
      "1    SEBACO     11_ocupados_x_condicion.csv       utf-8         ,\n",
      "2    SEBACO   22_participacion_expo_sbc.csv       ascii         ,\n",
      "3    SEBACO          05_empleo_sbc_oede.csv       ascii         ,\n",
      "4    SEBACO      16_brecha_salarial_ssi.csv       utf-8         ,\n",
      "..      ...                             ...         ...       ...\n",
      "332  MINERI       minerales_vbp_mundial.csv  ISO-8859-1         ,\n",
      "333  MINERI    importaciones_por_sector.csv       ascii         ,\n",
      "334  MINERI                  pbi_minero.csv       ascii         ,\n",
      "335  MINERI     importaciones_por_grupo.csv       ascii         ,\n",
      "336  MINERI  compras_sector_minero_rama.csv       utf-8         ,\n",
      "\n",
      "[337 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import chardet\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# ruta de trabajo donde estan los topicos con sus csv \n",
    "base_dir =  '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata' # '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata' #\n",
    "\n",
    "# lista para recolectar los datos\n",
    "data = []\n",
    "\n",
    "# tamanio del fragmento a leer (en este caso 100000 bytes)\n",
    "sample_size = 100000\n",
    "\n",
    "for topico in os.listdir(base_dir): # ejemplo SEBACO\n",
    "    topico_path = os.path.join(base_dir, topico) #ejemplo /home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata/SEBACO\n",
    "\n",
    "    # ignorar archivos sueltos como LICENSE y README.md\n",
    "    if not os.path.isdir(topico_path):\n",
    "        continue\n",
    "\n",
    "    for archivo in os.listdir(topico_path):\n",
    "        if archivo.endswith('.csv'):\n",
    "            archivo_path = os.path.join(topico_path, archivo)\n",
    "\n",
    "            # leer un fragmento del .csv para análisis\n",
    "            with open(archivo_path, 'rb') as f:\n",
    "                raw_sample = f.read(sample_size)\n",
    "\n",
    "            # detectar encoding\n",
    "            encoding_detected = 'utf-8'  # por defecto\n",
    "            try:\n",
    "                encoding_info = chardet.detect(raw_sample)\n",
    "                if encoding_info and encoding_info['encoding']:\n",
    "                    encoding_detected = encoding_info['encoding']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # decodificar el fragmento\n",
    "            try:\n",
    "                sample = raw_sample.decode(encoding_detected, errors = 'replace')\n",
    "            except:\n",
    "                sample = raw_sample.decode('utf-8', errors = 'replace') \n",
    "\n",
    "            # detectar delimitador\n",
    "            delimiter_detected = ','\n",
    "            try:\n",
    "                sniffer = csv.Sniffer()\n",
    "                dialect = sniffer.sniff(sample)\n",
    "                delimiter_detected = dialect.delimiter\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # agregar al resultado\n",
    "            data.append({\n",
    "                'TOPICO': topico,\n",
    "                'archivo_csv': archivo,\n",
    "                'encoding': encoding_detected,\n",
    "                'delimiter': delimiter_detected\n",
    "            })\n",
    "\n",
    "\n",
    "df_resultado = pd.DataFrame(data)\n",
    "df_codificacion = df_resultado\n",
    "\n",
    "print(df_codificacion)\n",
    "\n",
    "# guardar como csv el archivo csvTopicoArgendata_encoding_and_delimiters\n",
    "# path_df_encoding_delimiters = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/'\n",
    "# df_resultado.to_csv(path_df_encoding_delimiters + 'csvTopicosArgendata_codificacion_y_delimitadores.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <u> <font color = \"orangered\">  Tareas concretas del proceso de renaming </font> </u>\n",
    "\n",
    "<b> <font color = \"gold\"> Sobre argendata/data lo que voy a hacer es tomar los .csv  originales y hacer lo siguiente (dependiendo el caso), generando .csv actualizados: </font> </b>\n",
    "\n",
    "   - si un dataset tiene un código que esté incluido en los geocodigos del geonomenclador, se nomencla con la string de name_long (si es que existe), y si no es así se nomencla con la string de desc_fundar correspondiente\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que matchee sin ambigüedad en desc_fundar, entonces se usa el código que está en el geonomenclador (y se guarda la asociación del codigo encontrado con el codigo usado)\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que NO matchea sin ambigüedad en desc_fundar, se genera un código nuevo (a revisión para ser desambiguado con posibles matches)\n",
    "\n",
    "   - Si un dataset tiene una string que matchee sin ambigüedad en desc_fundar entonces se usa el código que está en el geonomenclador para esa string\n",
    "\n",
    "   - Si un dataset tiene una string que NO matchee sin ambigüedad en desc_fundar, entonces se genera un código nuevo (a revisión para ser desambiguado con posibles matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"gold\"> Para hacer esto, empecemos por: </font> </b>\n",
    "\n",
    "1. abriría el archivo según su encoding y delimiter dado por df_codificacion \n",
    "\n",
    "2. detectaría si cada .csv de tiene una columna que se corresponda a la columna geocodigo del csv geonomenclador según la siguiente condición: fijarse si, al menos, el 80% de sus filas se encuentra contenida en la columna geocodigo del geonomenclador\n",
    "\n",
    "3. detectaría si cada .csv de tiene una columna que se corresponda a la columna desc_fundar del csv geonomenclador según la siguiente condición: fijarse si, al menos, el 80% de sus filas se encuentra contenida en la columna desc_fundar del geonomenclador\n",
    "\n",
    "4. esta información guardarla en un dataframe llamado columnscsv_Geocodigo_descFundar, con formato de 4 columnas que sean 'TOPICO', 'archivo_csv', 'columna_Geocodigo', 'columna_DescFundar'. Donde sea 'columna_Geocodigo' es el nombre de la columna de dicho cvs que cumple la condición 60% (80%, X%) geocodigo del geonomenclador y 'descFundar' es el nombre de la columna de dicho csv que cumple la condición 60% (80%, X%) desc_fundar del geonomenclador. \n",
    "\n",
    "A la hora de hacer la comparación del 60%:\n",
    "- De la columna analizada la columna, eliminar nulos\n",
    "- normalizar espacios, mayúsculas, tildes, para que casos como los siguientes no rompan el X%: \"NEA \" y \"NEA\" o \"pampeana\" y \"Pampeana\" o \"Córdoba\" y \"Córdoba\" se reconozcan como idénticos\n",
    "\n",
    "<b> <font color = \"bisque\"> Esto resulta en un df con: TOPICO | archivo_csv | columna_Geocodigo | columna_DescFundar </font> </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOPICO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "archivo_csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "columna_Geocodigo",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "columna_DescFundar",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "b86562ed-bf22-4f14-93c1-d135d7431863",
       "rows": [
        [
         "0",
         "SEBACO",
         "01_ventas_afip.csv",
         null,
         null
        ],
        [
         "1",
         "SEBACO",
         "11_ocupados_x_condicion.csv",
         null,
         null
        ],
        [
         "2",
         "SEBACO",
         "22_participacion_expo_sbc.csv",
         "iso3",
         null
        ],
        [
         "3",
         "SEBACO",
         "05_empleo_sbc_oede.csv",
         null,
         null
        ],
        [
         "4",
         "SEBACO",
         "16_brecha_salarial_ssi.csv",
         null,
         null
        ],
        [
         "5",
         "SEBACO",
         "17_tasa_teletrabajo.csv",
         null,
         null
        ],
        [
         "6",
         "SEBACO",
         "20_composicion_expo_sbc.csv",
         null,
         null
        ],
        [
         "7",
         "SEBACO",
         "12_salarios_sbc_y_desagregado.csv",
         null,
         null
        ],
        [
         "8",
         "SEBACO",
         "04_exportacion_sbc.csv",
         null,
         null
        ],
        [
         "9",
         "SEBACO",
         "25_impo_x_socio_comercial_arg.csv",
         "iso3",
         null
        ],
        [
         "10",
         "SEBACO",
         "18_expo_impo_saldo_x_sector.csv",
         null,
         null
        ],
        [
         "11",
         "SEBACO",
         "09_composicion_sbc_rama.csv",
         null,
         null
        ],
        [
         "12",
         "SEBACO",
         "02_vbp_ssi_fundar.csv",
         null,
         null
        ],
        [
         "13",
         "SEBACO",
         "23_participacion_expo_ssi.csv",
         "iso3",
         null
        ],
        [
         "14",
         "SEBACO",
         "10_empleo_x_rama_y_prop.csv",
         null,
         null
        ],
        [
         "15",
         "SEBACO",
         "14_participacion_femenina_sbc.csv",
         null,
         null
        ],
        [
         "16",
         "SEBACO",
         "21_exportaciones_ssi.csv",
         null,
         null
        ],
        [
         "17",
         "SEBACO",
         "19_saldo_comex_x_sector.csv",
         null,
         null
        ],
        [
         "18",
         "SEBACO",
         "03_vab_indec.csv",
         null,
         null
        ],
        [
         "19",
         "SEBACO",
         "15_participacion_femenina_ssi.csv",
         null,
         null
        ],
        [
         "20",
         "SEBACO",
         "13_ocupados_x_niveled.csv",
         null,
         null
        ],
        [
         "21",
         "SEBACO",
         "07_empleo_sbc_provincias_princ.csv",
         null,
         null
        ],
        [
         "22",
         "SEBACO",
         "06_empleo_sbc_provincia.csv",
         null,
         "provincia"
        ],
        [
         "23",
         "SEBACO",
         "08_i_d_sbc.csv",
         null,
         null
        ],
        [
         "24",
         "SEBACO",
         "24_expo_x_socio_comercial_arg.csv",
         "iso3",
         null
        ],
        [
         "25",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "iso3",
         "iso3_desc_fundar"
        ],
        [
         "26",
         "ESTPRO",
         "densidad_empresarial_depto.csv",
         null,
         "provincia"
        ],
        [
         "27",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "iso3",
         "iso3_desc_fundar"
        ],
        [
         "28",
         "ESTPRO",
         "particip_sectorial_empleo.csv",
         null,
         null
        ],
        [
         "29",
         "ESTPRO",
         "va_por_trabajador_pais.csv",
         "iso3",
         "iso3_desc_fundar"
        ],
        [
         "30",
         "ESTPRO",
         "escala_salarios.csv",
         null,
         null
        ],
        [
         "31",
         "ESTPRO",
         "tasa_informalidad_tamanio_empresa.csv",
         null,
         null
        ],
        [
         "32",
         "ESTPRO",
         "vab_sectorial_provincia.csv",
         null,
         "provincia"
        ],
        [
         "33",
         "ESTPRO",
         "empresas_nacional.csv",
         null,
         null
        ],
        [
         "34",
         "ESTPRO",
         "particip_mujer.csv",
         null,
         null
        ],
        [
         "35",
         "ESTPRO",
         "productividad_internacional.csv",
         "iso3",
         null
        ],
        [
         "36",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "iso3",
         "iso3_desc_fundar"
        ],
        [
         "37",
         "ESTPRO",
         "empleo_invdes_tim_ocde.csv",
         "iso3",
         null
        ],
        [
         "38",
         "ESTPRO",
         "pib_por_ocupado.csv",
         "iso3",
         "pais_nombre"
        ],
        [
         "39",
         "ESTPRO",
         "sector_escala.csv",
         null,
         null
        ],
        [
         "40",
         "ESTPRO",
         "empn_tim_ocde.csv",
         "iso3",
         null
        ],
        [
         "41",
         "ESTPRO",
         "particip_mujer_varon.csv",
         null,
         null
        ],
        [
         "42",
         "ESTPRO",
         "empleo_sectores_ggdc_1950_2018.csv",
         "iso3",
         null
        ],
        [
         "43",
         "ESTPRO",
         "particip_calificados_sector.csv",
         null,
         null
        ],
        [
         "44",
         "ESTPRO",
         "vab_sectorial_granregion.csv",
         null,
         null
        ],
        [
         "45",
         "ESTPRO",
         "densidad_nbi.csv",
         null,
         "provincia"
        ],
        [
         "46",
         "ESTPRO",
         "va_por_trabajador.csv",
         null,
         null
        ],
        [
         "47",
         "ESTPRO",
         "vab_por_sector.csv",
         null,
         null
        ],
        [
         "48",
         "ESTPRO",
         "particip_sectorial_empleoyvab.csv",
         null,
         null
        ],
        [
         "49",
         "ESTPRO",
         "particip_sector_vab.csv",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 337
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICO</th>\n",
       "      <th>archivo_csv</th>\n",
       "      <th>columna_Geocodigo</th>\n",
       "      <th>columna_DescFundar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>01_ventas_afip.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>11_ocupados_x_condicion.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>22_participacion_expo_sbc.csv</td>\n",
       "      <td>iso3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>05_empleo_sbc_oede.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>16_brecha_salarial_ssi.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>minerales_vbp_mundial.csv</td>\n",
       "      <td>codigo_pais</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>importaciones_por_sector.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>pbi_minero.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>importaciones_por_grupo.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>compras_sector_minero_rama.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TOPICO                     archivo_csv columna_Geocodigo  \\\n",
       "0    SEBACO              01_ventas_afip.csv              None   \n",
       "1    SEBACO     11_ocupados_x_condicion.csv              None   \n",
       "2    SEBACO   22_participacion_expo_sbc.csv              iso3   \n",
       "3    SEBACO          05_empleo_sbc_oede.csv              None   \n",
       "4    SEBACO      16_brecha_salarial_ssi.csv              None   \n",
       "..      ...                             ...               ...   \n",
       "332  MINERI       minerales_vbp_mundial.csv       codigo_pais   \n",
       "333  MINERI    importaciones_por_sector.csv              None   \n",
       "334  MINERI                  pbi_minero.csv              None   \n",
       "335  MINERI     importaciones_por_grupo.csv              None   \n",
       "336  MINERI  compras_sector_minero_rama.csv              None   \n",
       "\n",
       "    columna_DescFundar  \n",
       "0                 None  \n",
       "1                 None  \n",
       "2                 None  \n",
       "3                 None  \n",
       "4                 None  \n",
       "..                 ...  \n",
       "332               None  \n",
       "333               None  \n",
       "334               None  \n",
       "335               None  \n",
       "336               None  \n",
       "\n",
       "[337 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion para eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# rutas y dataframes necesarios\n",
    "\n",
    "base_dir = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata' # '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata' \n",
    "# df_codificacion hecho\n",
    "df_geonomenclador = pd.read_csv('/home/lucia/Desktop/Fundar/Argendata_renaming/data/input' + '/geonomenclador_LuloTest.csv')\n",
    "\n",
    "# sets normalizados de las columnas de interes del geonomenclador: geocodigo y desc_fundar \n",
    "# estoy probndo usar set para practicar, si es necesario paso a lista\n",
    "set_geocod = set(\n",
    "    df_geonomenclador['geocodigo']\n",
    "      .dropna().astype(str) #elimino nulos\n",
    "      .str.strip() # quitar espacios al inicio/final\n",
    "      .str.upper() # pasar todo a mayusculas\n",
    "      .apply(sin_tildes) # sacar tildes\n",
    ")\n",
    "set_desc = set(\n",
    "    df_geonomenclador['desc_fundar']\n",
    "      .dropna().astype(str)\n",
    "      .str.strip()\n",
    "      .str.upper()\n",
    "      .apply(sin_tildes)\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "# los topicos (directorios) ignorando los archivos LICENSE y README.md\n",
    "topics = []\n",
    "for d in os.listdir(base_dir):\n",
    "    item_path = os.path.join(base_dir, d)\n",
    "    \n",
    "    if os.path.isdir(item_path): # verifico que sea un directorio\n",
    "        \n",
    "        if d != 'LICENSE' and d != 'README.md': # no incluyo LICENSE y README.md\n",
    "            topics.append(d)\n",
    "\n",
    "for topico in topics:\n",
    "    topic_dir = os.path.join(base_dir, topico)\n",
    "    # listo los .csv de este tópico\n",
    "    csv_files = [f for f in os.listdir(topic_dir) if f.endswith('.csv')]\n",
    "    for archivo in csv_files:\n",
    "        # uso el encoding y delimiter de df_codificacion\n",
    "        row = df_codificacion[\n",
    "            (df_codificacion['TOPICO'] == topico) &\n",
    "            (df_codificacion['archivo_csv'] == archivo)\n",
    "        ]\n",
    "        if row.empty:\n",
    "            print(f'No hay info de encoding/delimiter para {topico}/{archivo}') # quiero probar no estar pifiando\n",
    "            continue\n",
    "        encoding  = row.iloc[0]['encoding']\n",
    "        delimiter = row.iloc[0]['delimiter']\n",
    "        path_csv  = os.path.join(topic_dir, archivo)\n",
    "\n",
    "        # leo csv con el encoding delimiter\n",
    "        try:\n",
    "            df_csv = pd.read_csv(path_csv, encoding = encoding, delimiter = delimiter)\n",
    "        except Exception as e: \n",
    "            print(f'Error leyendo {path_csv}: {e}') # imprimo el error\n",
    "            continue\n",
    "\n",
    "        col_geocod = None\n",
    "        col_desc   = None\n",
    "\n",
    "        # analizar cada columna del .csv\n",
    "        for col in df_csv.columns:\n",
    "            serie = (\n",
    "                df_csv[col]\n",
    "                  .dropna().astype(str)\n",
    "                  .str.strip()\n",
    "                  .str.upper()\n",
    "                  .apply(sin_tildes)\n",
    "            )\n",
    "            n = len(serie)\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            match_geo  = serie.isin(set_geocod).sum() / n\n",
    "            match_desc = serie.isin(set_desc).sum()   / n\n",
    "\n",
    "            if match_geo  >= 0.6 and col_geocod is None:\n",
    "                col_geocod = col\n",
    "            if match_desc >= 0.6 and col_desc   is None:\n",
    "                col_desc = col\n",
    "\n",
    "        results.append({\n",
    "            'TOPICO': topico,\n",
    "            'archivo_csv': archivo,\n",
    "            'columna_Geocodigo': col_geocod,\n",
    "            'columna_DescFundar': col_desc\n",
    "        })\n",
    "\n",
    "# creo df de interes\n",
    "columnscsv_Geocodigo_descFundar = pd.DataFrame(results)\n",
    "\n",
    "columnscsv_Geocodigo_descFundar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "geocodigo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "desc_fundar",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name_long",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "name_short",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Unnamed: 4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 7",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "3717a7a5-458e-4970-b488-99aaebb4e870",
       "rows": [
        [
         "0",
         "ABW",
         "Aruba",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "AFE",
         "África Oriental y Meridional",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "AFG",
         "Afganistán",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "AFW",
         "África Occidental y Central",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "AGO",
         "Angola",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "AIA",
         "Anguila",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "ALA",
         "Islas Åland",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "ALB",
         "Albania",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "AND",
         "Andorra",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "ANT",
         "Antillas Holandesas",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "ARB",
         "Mundo Árabe",
         "Mundo árabe",
         "Mundo árabe",
         null,
         null,
         null,
         " "
        ],
        [
         "11",
         "ARE",
         "Emiratos Árabes Unidos",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "ARG",
         "Argentina",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "ARM",
         "Armenia",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "ASM",
         "Samoa Americana",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "ATA",
         "Antártida",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "ATF",
         "Territorio de las Tierras Australes Francesas",
         "Tierras Australes Francesas",
         "Tierras Australes Francesas",
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "ATG",
         "Antigua y Barbuda",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "AUS",
         "Australia",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "AUT",
         "Austria",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "AZE",
         "Azerbaiyán",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "BAT",
         "Territorio Antártico Británico",
         "Antártida Británica",
         "Antártida Británica",
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "BDI",
         "Burundi",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "BEL",
         "Bélgica",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "BEN",
         "Benin",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "BES",
         "Bonaire, San Eustaquio y Saba",
         "Bonaire",
         "Bonaire",
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "BFA",
         "Burkina Faso",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "BGD",
         "Bangladesh",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "BGR",
         "Bulgaria",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "BHR",
         "Bahrein",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "BHS",
         "Bahamas",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "BIH",
         "Bosnia y Herzegovina",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "BLM",
         "San Barthélemy",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "33",
         "BLR",
         "Belarús",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "34",
         "BLX",
         "Bélgica-Luxemburgo",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "BLZ",
         "Belice",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "BMU",
         "Bermuda",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "BOL",
         "Bolivia",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "BRA",
         "Brasil",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "BRB",
         "Barbados",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "BRN",
         "Brunei Darussalam",
         "Brunei",
         "Brunei",
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "BTN",
         "Bhután",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "BVT",
         "Isla Bouvet",
         "Bouvet",
         "Bouvet",
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "BWA",
         "Botswana",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "CAF",
         "República Centroafricana",
         "Rep. Centroafricana",
         "Rep. Centroafricana",
         null,
         null,
         null,
         null
        ],
        [
         "45",
         "CAN",
         "Canadá",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "CCK",
         "Islas Cocos (Keeling)",
         "Islas Cocos",
         "Islas Cocos",
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "CEB",
         "Europa Central y Los Países Bálticos",
         "Europa Central y Bálticos",
         "Europa Central y Bálticos",
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "CEM",
         "Comunidad Económica y Monetaria de Africa Central (CEMAC)",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "CHE",
         "Suiza",
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 1024
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geocodigo</th>\n",
       "      <th>desc_fundar</th>\n",
       "      <th>name_long</th>\n",
       "      <th>name_short</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFE</td>\n",
       "      <td>África Oriental y Meridional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afganistán</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFW</td>\n",
       "      <td>África Occidental y Central</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGO</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>DESHUM_AHDI.WOF</td>\n",
       "      <td>Ramificaciones de Occidente (AHDI)</td>\n",
       "      <td>Ramificaciones de Occidente</td>\n",
       "      <td>Ramificaciones de Occidente</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>NIR</td>\n",
       "      <td>Irlanda del Norte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>CAMCLI_IA</td>\n",
       "      <td>Aviación internacional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>F_ESTPRO</td>\n",
       "      <td>África (OECD)</td>\n",
       "      <td>África</td>\n",
       "      <td>África</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>NAFTA_ESTPRO</td>\n",
       "      <td>Países miembros del NAFTA (OECD)</td>\n",
       "      <td>Países miembros del NAFTA</td>\n",
       "      <td>Países miembros del NAFTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            geocodigo                         desc_fundar  \\\n",
       "0                 ABW                               Aruba   \n",
       "1                 AFE        África Oriental y Meridional   \n",
       "2                 AFG                          Afganistán   \n",
       "3                 AFW         África Occidental y Central   \n",
       "4                 AGO                              Angola   \n",
       "...               ...                                 ...   \n",
       "1019  DESHUM_AHDI.WOF  Ramificaciones de Occidente (AHDI)   \n",
       "1020              NIR                   Irlanda del Norte   \n",
       "1021        CAMCLI_IA              Aviación internacional   \n",
       "1022         F_ESTPRO                       África (OECD)   \n",
       "1023     NAFTA_ESTPRO    Países miembros del NAFTA (OECD)   \n",
       "\n",
       "                        name_long                   name_short  Unnamed: 4  \\\n",
       "0                             NaN                          NaN         NaN   \n",
       "1                             NaN                          NaN         NaN   \n",
       "2                             NaN                          NaN         NaN   \n",
       "3                             NaN                          NaN         NaN   \n",
       "4                             NaN                          NaN         NaN   \n",
       "...                           ...                          ...         ...   \n",
       "1019  Ramificaciones de Occidente  Ramificaciones de Occidente         NaN   \n",
       "1020                          NaN                          NaN         NaN   \n",
       "1021                          NaN                          NaN         NaN   \n",
       "1022                       África                       África         NaN   \n",
       "1023    Países miembros del NAFTA    Países miembros del NAFTA         NaN   \n",
       "\n",
       "      Unnamed: 5  Unnamed: 6 Unnamed: 7  \n",
       "0            NaN         NaN        NaN  \n",
       "1            NaN         NaN        NaN  \n",
       "2            NaN         NaN        NaN  \n",
       "3            NaN         NaN        NaN  \n",
       "4            NaN         NaN        NaN  \n",
       "...          ...         ...        ...  \n",
       "1019         NaN         NaN        NaN  \n",
       "1020         NaN         NaN        NaN  \n",
       "1021         NaN         NaN        NaN  \n",
       "1022         NaN         NaN        NaN  \n",
       "1023         NaN         NaN        NaN  \n",
       "\n",
       "[1024 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geonomenclador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"gold\"> Continuamos con lo que enumero a continuación para cumplir las condiciones detalladas enteriormente (los 5 bullets de más arriba): </font> </b>\n",
    "\n",
    "Ya generamos el df columnscsv_Geocodigo_descFundar con: TOPICO | archivo_csv | columna_Geocodigo | columna_DescFundar, \n",
    "Donde 'columna_Geocodigo' es el nombre de la columna de dicho cvs que cumple la condición X% geocodigo del geonomenclador y 'descFundar' es el nombre de la columna de dicho csv que cumple la condición X% desc_fundar del geonomenclador.\n",
    "\n",
    "<b> <font color = \"gold\">Por cada tópico (carpeta) y cada csv dentro de este, busco generar una nueva carpeta de cada tópico y dentro de cada carpeta guardar una copia de sus respectivos csvs pero agregandoles a los mismos dos columnas: una que sea 'geocodigoFundar' y otra que sea 'geonombreFundar'.</font> </b>\n",
    "\n",
    "Si para un csv de un topico tanto columna_Geocodigo como columna_DescFundar es NA, ya sabremos que el csv copia se guardará con NA tanto en las columnas 'geocodigoFundar' como 'geonombreFundar'.\n",
    "\n",
    "Siendo que columnscsv_Geocodigo_descFundar ya tiene por cada tópico y csv, si dicho csv tiene una columna 'columna_Geocodigo' que sería de geocodigo (condicion X% de columna geocodigo del df_geonomenclador) y otra columna 'columna_DescFundar' (condicion del 80 % de la columna desc_fundar del df_geonomenclador), voy a usarcolumnscsv_Geocodigo_descFundar para completar 'geocodigoFundar' y 'geonombreFundar'.\n",
    "\n",
    "Para rellenar las columnas 'geocodigoFundar' y 'geonombreFundar', hay que tener las siguientes consideraciones:\n",
    "\n",
    "- Si un dataset (cada csv original de cada topico) tiene una columna asociada a geocodigo segun columnscsv_Geocodigo_descFundar, el contenido de la columna geocodigoFundar del csv copia tendrá el contenido de la columna asociada a geocodigo segun columnscsv_Geocodigo_descFundar. La columna geonombreFundar del csv copia llenará cada una de sus filas acorde al name_long (si es que existe), y si no es así al string de desc_fundar correspondiente a ese geocodigo del geonomenclador.\n",
    "\n",
    "- Si un dataset tiene una string que matchee sin ambigüedad en desc_fundar del geonomenclador entonces se usa el código que está en el geonomenclador para esa string. Similar al caso anterior.\n",
    "\n",
    "- En el caso de que un dataset tiene un código que NO esté incluido en los geocodigos pero tiene una string que matchee sin ambigüedad en desc_fundar, entonces se usa el código que está en geocódigo en el geonomenclador. Esto se registrará en df_problemas con las columnas TOPICO | csv | problema: \"dataset tiene el codigo [...], que NO esta incluido en los geocodigos pero tiene la string [...] que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador.\". Por cada problema se guarda la string y el código correspondiente. Esto puede dar lugar que un mismo csv de un tópico tenga varios problemas a registrar. NO HAY QUE MODIFICAR GEONOMENCLADOR, EL RENAMING ES HECHO A PARTIR DE LA COLUMNA desc_fundar DEL GEONOMENCLADOR. \n",
    "\n",
    "- Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que NO matchea sin ambigüedad en desc_fundar, se genera un código nuevo. Esto se registrará en df_problemas con las columnas TOPICO | csv | problema: \"dataset tiene el codigo [...] que NO esta incluido en los geocodigos y tiene la string [...] que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos).\". Por cada problema se guarda la string y el código correspondiente. Esto puede dar lugar que un mismo csv de un tópico tenga varios problemas a registrar. PODRÍA DAR LUGAR A MODIFICACIONES DEL GEONOMENCLADOR. \n",
    "\n",
    "    -Situacion similar a esta: Dataset tiene el código [...] que NO está incluido en los geocódigos y no existe columna desc_fundar. En df_problemas se reporta: \"dataset tiene el codigo [...] que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos).\"\n",
    "\n",
    "- Si un dataset tiene una string que NO matchee sin ambigüedad en desc_fundar, entonces guardar este caso en el dataframe llamado df_problemas con las columnas TOPICO | csv | problema, donde TOPICO es el topico con el que se corresponde, csv el csv evaluado y en problema poner: \"Para la string [...] se debería generar un codigo nuevo (a revisión, para no pisarse con otros casos)\". PODRÍA DAR LUGAR A MODIFICACIONES DEL GEONOMENCLADOR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"bisque\"> VAMOS CON ALGUNOS EJEMPLOS PARTICULARES </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iso3        iso3_desc_fundar       valor geocodigoFundar geonombreFundar\n",
      "0    AFG              Afganistán    28480.02            <NA>            <NA>\n",
      "1    AGO                  Angola    55384.74            <NA>            <NA>\n",
      "2    ALB                 Albania    13250.00            <NA>            <NA>\n",
      "3    ARE  Emiratos Árabes Unidos    46494.00            <NA>            <NA>\n",
      "4    ARG               Argentina  2294101.41            <NA>            <NA>\n",
      "..   ...                     ...         ...             ...             ...\n",
      "190  WSM                   Samoa      403.33            <NA>            <NA>\n",
      "191  YEM                   Yemen   198878.58            <NA>            <NA>\n",
      "192  ZAF               Sudáfrica  1910000.00            <NA>            <NA>\n",
      "193  ZMB                  Zambia    50533.34            <NA>            <NA>\n",
      "194  ZWE                Zimbabwe   113000.00            <NA>            <NA>\n",
      "\n",
      "[195 rows x 5 columns]\n",
      "iso3\n",
      "    iso3        iso3_desc_fundar       valor geocodigoFundar  \\\n",
      "0    AFG              Afganistán    28480.02             AFG   \n",
      "1    AGO                  Angola    55384.74             AGO   \n",
      "2    ALB                 Albania    13250.00             ALB   \n",
      "3    ARE  Emiratos Árabes Unidos    46494.00             ARE   \n",
      "4    ARG               Argentina  2294101.41             ARG   \n",
      "..   ...                     ...         ...             ...   \n",
      "190  WSM                   Samoa      403.33             WSM   \n",
      "191  YEM                   Yemen   198878.58             YEM   \n",
      "192  ZAF               Sudáfrica  1910000.00             ZAF   \n",
      "193  ZMB                  Zambia    50533.34             ZMB   \n",
      "194  ZWE                Zimbabwe   113000.00             ZWE   \n",
      "\n",
      "            geonombreFundar  \n",
      "0                Afganistán  \n",
      "1                    Angola  \n",
      "2                   Albania  \n",
      "3    Emiratos Árabes Unidos  \n",
      "4                 Argentina  \n",
      "..                      ...  \n",
      "190                   Samoa  \n",
      "191                   Yemen  \n",
      "192               Sudáfrica  \n",
      "193                  Zambia  \n",
      "194                Zimbabwe  \n",
      "\n",
      "[195 rows x 5 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE UN CSV DE UN TOPICO ARGENDATA QUE ESPERO QUE NO TENGA PROBLEMAS: SE GENEROOOOO!!!! :D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'AGROPE'  \n",
    "ARCHIVO   = 'produccion_global_carne_aviar_2021.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prov_cod            prov_desc  tasa_empleo_18_65_mujeres  \\\n",
      "0          2                 CABA                   0.755180   \n",
      "1          6         Buenos Aires                   0.625285   \n",
      "2         10            Catamarca                   0.590940   \n",
      "3         14              Córdoba                   0.585850   \n",
      "4         18           Corrientes                   0.482550   \n",
      "5         22                Chaco                   0.505072   \n",
      "6         26               Chubut                   0.594072   \n",
      "7         30           Entre Ríos                   0.569709   \n",
      "8         34              Formosa                   0.421378   \n",
      "9         38                Jujuy                   0.623202   \n",
      "10        42             La Pampa                   0.623782   \n",
      "11        46             La Rioja                   0.585750   \n",
      "12        50              Mendoza                   0.588962   \n",
      "13        54             Misiones                   0.611542   \n",
      "14        58              Neuquén                   0.599055   \n",
      "15        62            Río Negro                   0.542795   \n",
      "16        66                Salta                   0.595649   \n",
      "17        70             San Juan                   0.515784   \n",
      "18        74             San Luis                   0.622475   \n",
      "19        78           Santa Cruz                   0.619858   \n",
      "20        82             Santa Fe                   0.614329   \n",
      "21        86  Santiago del Estero                   0.552190   \n",
      "22        90              Tucumán                   0.598329   \n",
      "23        94     Tierra del Fuego                   0.608843   \n",
      "\n",
      "    prop_usa_lavarropas geocodigoFundar geonombreFundar  \n",
      "0              0.766963            <NA>            <NA>  \n",
      "1              0.692271            <NA>            <NA>  \n",
      "2              0.565825            <NA>            <NA>  \n",
      "3              0.723312            <NA>            <NA>  \n",
      "4              0.506944            <NA>            <NA>  \n",
      "5              0.430127            <NA>            <NA>  \n",
      "6              0.814129            <NA>            <NA>  \n",
      "7              0.631294            <NA>            <NA>  \n",
      "8              0.415525            <NA>            <NA>  \n",
      "9              0.615616            <NA>            <NA>  \n",
      "10             0.797228            <NA>            <NA>  \n",
      "11             0.701220            <NA>            <NA>  \n",
      "12             0.843693            <NA>            <NA>  \n",
      "13             0.548459            <NA>            <NA>  \n",
      "14             0.793524            <NA>            <NA>  \n",
      "15             0.734564            <NA>            <NA>  \n",
      "16             0.547347            <NA>            <NA>  \n",
      "17             0.745884            <NA>            <NA>  \n",
      "18             0.789810            <NA>            <NA>  \n",
      "19             0.842580            <NA>            <NA>  \n",
      "20             0.666890            <NA>            <NA>  \n",
      "21             0.328093            <NA>            <NA>  \n",
      "22             0.437296            <NA>            <NA>  \n",
      "23             0.916818            <NA>            <NA>  \n",
      "None\n",
      "    prov_cod            prov_desc  tasa_empleo_18_65_mujeres  \\\n",
      "0          2                 CABA                   0.755180   \n",
      "1          6         Buenos Aires                   0.625285   \n",
      "2         10            Catamarca                   0.590940   \n",
      "3         14              Córdoba                   0.585850   \n",
      "4         18           Corrientes                   0.482550   \n",
      "5         22                Chaco                   0.505072   \n",
      "6         26               Chubut                   0.594072   \n",
      "7         30           Entre Ríos                   0.569709   \n",
      "8         34              Formosa                   0.421378   \n",
      "9         38                Jujuy                   0.623202   \n",
      "10        42             La Pampa                   0.623782   \n",
      "11        46             La Rioja                   0.585750   \n",
      "12        50              Mendoza                   0.588962   \n",
      "13        54             Misiones                   0.611542   \n",
      "14        58              Neuquén                   0.599055   \n",
      "15        62            Río Negro                   0.542795   \n",
      "16        66                Salta                   0.595649   \n",
      "17        70             San Juan                   0.515784   \n",
      "18        74             San Luis                   0.622475   \n",
      "19        78           Santa Cruz                   0.619858   \n",
      "20        82             Santa Fe                   0.614329   \n",
      "21        86  Santiago del Estero                   0.552190   \n",
      "22        90              Tucumán                   0.598329   \n",
      "23        94     Tierra del Fuego                   0.608843   \n",
      "\n",
      "    prop_usa_lavarropas geocodigoFundar      geonombreFundar  \n",
      "0              0.766963            <NA>                 <NA>  \n",
      "1              0.692271            AR-B         Buenos Aires  \n",
      "2              0.565825            AR-K            Catamarca  \n",
      "3              0.723312            AR-X              Córdoba  \n",
      "4              0.506944            AR-W           Corrientes  \n",
      "5              0.430127            AR-H                Chaco  \n",
      "6              0.814129            AR-U               Chubut  \n",
      "7              0.631294            AR-E           Entre Ríos  \n",
      "8              0.415525            AR-P              Formosa  \n",
      "9              0.615616            AR-Y                Jujuy  \n",
      "10             0.797228            AR-L             La Pampa  \n",
      "11             0.701220            AR-F             La Rioja  \n",
      "12             0.843693            AR-M              Mendoza  \n",
      "13             0.548459            AR-N             Misiones  \n",
      "14             0.793524            AR-Q              Neuquén  \n",
      "15             0.734564            AR-R            Río Negro  \n",
      "16             0.547347            AR-A                Salta  \n",
      "17             0.745884            AR-J             San Juan  \n",
      "18             0.789810            AR-D             San Luis  \n",
      "19             0.842580            AR-Z           Santa Cruz  \n",
      "20             0.666890            AR-S             Santa Fe  \n",
      "21             0.328093            AR-G  Santiago del Estero  \n",
      "22             0.437296            AR-T              Tucumán  \n",
      "23             0.916818            AR-V     Tierra del Fuego  \n",
      "   TOPICO                                       csv  \\\n",
      "0  MERTRA  lavarropas_tasa_empleo_fem_provincia.csv   \n",
      "\n",
      "                                            problema  \n",
      "0  Para la string 'CABA' generar un codigo nuevo ...  \n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'MERTRA'  \n",
    "# ARCHIVO   = 'lavarropas_tasa_empleo_fem_provincia.csv'\n",
    "# Para la string 'CABA' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'MERTRA'  \n",
    "ARCHIVO   = 'lavarropas_tasa_empleo_fem_provincia.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # CASO B: no existe columna de geocodigo, pero si se tiene de desc_fundar\n",
    "\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # CASO C: no hay columnas correspondientes a geocodigo ni de desc_fundar, queda NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso3                    iso3_desc  anio  \\\n",
      "0      AFE  Africa Eastern and Southern  2023   \n",
      "1      AFE  Africa Eastern and Southern  2022   \n",
      "2      AFE  Africa Eastern and Southern  2021   \n",
      "3      AFE  Africa Eastern and Southern  2020   \n",
      "4      AFE  Africa Eastern and Southern  2019   \n",
      "...    ...                          ...   ...   \n",
      "16699  ZWE                     Zimbabwe  1964   \n",
      "16700  ZWE                     Zimbabwe  1963   \n",
      "16701  ZWE                     Zimbabwe  1962   \n",
      "16702  ZWE                     Zimbabwe  1961   \n",
      "16703  ZWE                     Zimbabwe  1960   \n",
      "\n",
      "       ratio_tasa_actividad_mujer_varon nivel_agregacion geocodigoFundar  \\\n",
      "0                                   NaN       agregacion            <NA>   \n",
      "1                                   NaN       agregacion            <NA>   \n",
      "2                                   NaN       agregacion            <NA>   \n",
      "3                                   NaN       agregacion            <NA>   \n",
      "4                                   NaN       agregacion            <NA>   \n",
      "...                                 ...              ...             ...   \n",
      "16699                               NaN             pais            <NA>   \n",
      "16700                               NaN             pais            <NA>   \n",
      "16701                               NaN             pais            <NA>   \n",
      "16702                               NaN             pais            <NA>   \n",
      "16703                               NaN             pais            <NA>   \n",
      "\n",
      "      geonombreFundar  \n",
      "0                <NA>  \n",
      "1                <NA>  \n",
      "2                <NA>  \n",
      "3                <NA>  \n",
      "4                <NA>  \n",
      "...               ...  \n",
      "16699            <NA>  \n",
      "16700            <NA>  \n",
      "16701            <NA>  \n",
      "16702            <NA>  \n",
      "16703            <NA>  \n",
      "\n",
      "[16704 rows x 7 columns]\n",
      "iso3\n",
      "      iso3                    iso3_desc  anio  \\\n",
      "0      AFE  Africa Eastern and Southern  2023   \n",
      "1      AFE  Africa Eastern and Southern  2022   \n",
      "2      AFE  Africa Eastern and Southern  2021   \n",
      "3      AFE  Africa Eastern and Southern  2020   \n",
      "4      AFE  Africa Eastern and Southern  2019   \n",
      "...    ...                          ...   ...   \n",
      "16699  ZWE                     Zimbabwe  1964   \n",
      "16700  ZWE                     Zimbabwe  1963   \n",
      "16701  ZWE                     Zimbabwe  1962   \n",
      "16702  ZWE                     Zimbabwe  1961   \n",
      "16703  ZWE                     Zimbabwe  1960   \n",
      "\n",
      "       ratio_tasa_actividad_mujer_varon nivel_agregacion geocodigoFundar  \\\n",
      "0                                   NaN       agregacion             AFE   \n",
      "1                                   NaN       agregacion             AFE   \n",
      "2                                   NaN       agregacion             AFE   \n",
      "3                                   NaN       agregacion             AFE   \n",
      "4                                   NaN       agregacion             AFE   \n",
      "...                                 ...              ...             ...   \n",
      "16699                               NaN             pais             ZWE   \n",
      "16700                               NaN             pais             ZWE   \n",
      "16701                               NaN             pais             ZWE   \n",
      "16702                               NaN             pais             ZWE   \n",
      "16703                               NaN             pais             ZWE   \n",
      "\n",
      "                    geonombreFundar  \n",
      "0      África Oriental y Meridional  \n",
      "1      África Oriental y Meridional  \n",
      "2      África Oriental y Meridional  \n",
      "3      África Oriental y Meridional  \n",
      "4      África Oriental y Meridional  \n",
      "...                             ...  \n",
      "16699                      Zimbabwe  \n",
      "16700                      Zimbabwe  \n",
      "16701                      Zimbabwe  \n",
      "16702                      Zimbabwe  \n",
      "16703                      Zimbabwe  \n",
      "\n",
      "[16704 rows x 7 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'MERTRA'  \n",
    "# ARCHIVO   = 'ratio_tasa_actividad_mujer_varon_por_pais_anio.csv'\n",
    "# ya no tiene problemas :D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'MERTRA'  \n",
    "ARCHIVO   = 'ratio_tasa_actividad_mujer_varon_por_pais_anio.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     anio     pais_o_region  pbi_per_capita_ppa geocodigoFundar  \\\n",
      "0    1950    America Latina         3672.315179            <NA>   \n",
      "1    1951    America Latina         3772.801926            <NA>   \n",
      "2    1952    America Latina         3800.175320            <NA>   \n",
      "3    1953    America Latina         3852.794160            <NA>   \n",
      "4    1954    America Latina         3973.484863            <NA>   \n",
      "..    ...               ...                 ...             ...   \n",
      "214  2018  Tigres Asiaticos        42850.848420            <NA>   \n",
      "215  2019  Tigres Asiaticos        43538.234690            <NA>   \n",
      "216  2020  Tigres Asiaticos        43324.091790            <NA>   \n",
      "217  2021  Tigres Asiaticos        46071.454090            <NA>   \n",
      "218  2022  Tigres Asiaticos        47014.943250            <NA>   \n",
      "\n",
      "    geonombreFundar  \n",
      "0              <NA>  \n",
      "1              <NA>  \n",
      "2              <NA>  \n",
      "3              <NA>  \n",
      "4              <NA>  \n",
      "..              ...  \n",
      "214            <NA>  \n",
      "215            <NA>  \n",
      "216            <NA>  \n",
      "217            <NA>  \n",
      "218            <NA>  \n",
      "\n",
      "[219 rows x 5 columns]\n",
      "None\n",
      "     anio     pais_o_region  pbi_per_capita_ppa geocodigoFundar  \\\n",
      "0    1950    America Latina         3672.315179      DESIGU_AML   \n",
      "1    1951    America Latina         3772.801926      DESIGU_AML   \n",
      "2    1952    America Latina         3800.175320      DESIGU_AML   \n",
      "3    1953    America Latina         3852.794160      DESIGU_AML   \n",
      "4    1954    America Latina         3973.484863      DESIGU_AML   \n",
      "..    ...               ...                 ...             ...   \n",
      "214  2018  Tigres Asiaticos        42850.848420            <NA>   \n",
      "215  2019  Tigres Asiaticos        43538.234690            <NA>   \n",
      "216  2020  Tigres Asiaticos        43324.091790            <NA>   \n",
      "217  2021  Tigres Asiaticos        46071.454090            <NA>   \n",
      "218  2022  Tigres Asiaticos        47014.943250            <NA>   \n",
      "\n",
      "    geonombreFundar  \n",
      "0    América Latina  \n",
      "1    América Latina  \n",
      "2    América Latina  \n",
      "3    América Latina  \n",
      "4    América Latina  \n",
      "..              ...  \n",
      "214            <NA>  \n",
      "215            <NA>  \n",
      "216            <NA>  \n",
      "217            <NA>  \n",
      "218            <NA>  \n",
      "\n",
      "[219 rows x 5 columns]\n",
      "    TOPICO                       csv  \\\n",
      "0   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "1   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "2   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "3   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "4   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "..     ...                       ...   \n",
      "68  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "69  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "70  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "71  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "72  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "\n",
      "                                             problema  \n",
      "0   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "1   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "2   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "3   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "4   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "..                                                ...  \n",
      "68  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "69  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "70  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "71  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "72  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "\n",
      "[73 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'ACECON'  \n",
    "# ARCHIVO   = '9_pibpc_ppa_log_1950.csv'\n",
    "# Para la string 'TIGRES ASIATICOS' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'ACECON'  \n",
    "ARCHIVO   = '9_pibpc_ppa_log_1950.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     region_code                  region_name  year  poverty_line  \\\n",
      "0            AFE  Eastern and Southern Africa  1993          2.15   \n",
      "1            AFE  Eastern and Southern Africa  1994          2.15   \n",
      "2            AFE  Eastern and Southern Africa  1995          2.15   \n",
      "3            AFE  Eastern and Southern Africa  1996          2.15   \n",
      "4            AFE  Eastern and Southern Africa  1998          2.15   \n",
      "...          ...                          ...   ...           ...   \n",
      "1910         WLD                        World  2015         15.00   \n",
      "1911         WLD                        World  2016         15.00   \n",
      "1912         WLD                        World  2017         15.00   \n",
      "1913         WLD                        World  2018         15.00   \n",
      "1914         WLD                        World  2019         15.00   \n",
      "\n",
      "      poverty_rate geocodigoFundar geonombreFundar  \n",
      "0         0.584803            <NA>            <NA>  \n",
      "1         0.589474            <NA>            <NA>  \n",
      "2         0.579662            <NA>            <NA>  \n",
      "3         0.568109            <NA>            <NA>  \n",
      "4         0.569110            <NA>            <NA>  \n",
      "...            ...             ...             ...  \n",
      "1910      0.731318            <NA>            <NA>  \n",
      "1911      0.724939            <NA>            <NA>  \n",
      "1912      0.717605            <NA>            <NA>  \n",
      "1913      0.711307            <NA>            <NA>  \n",
      "1914      0.705482            <NA>            <NA>  \n",
      "\n",
      "[1915 rows x 7 columns]\n",
      "region_code\n",
      "     region_code                  region_name  year  poverty_line  \\\n",
      "0            AFE  Eastern and Southern Africa  1993          2.15   \n",
      "1            AFE  Eastern and Southern Africa  1994          2.15   \n",
      "2            AFE  Eastern and Southern Africa  1995          2.15   \n",
      "3            AFE  Eastern and Southern Africa  1996          2.15   \n",
      "4            AFE  Eastern and Southern Africa  1998          2.15   \n",
      "...          ...                          ...   ...           ...   \n",
      "1910         WLD                        World  2015         15.00   \n",
      "1911         WLD                        World  2016         15.00   \n",
      "1912         WLD                        World  2017         15.00   \n",
      "1913         WLD                        World  2018         15.00   \n",
      "1914         WLD                        World  2019         15.00   \n",
      "\n",
      "      poverty_rate geocodigoFundar               geonombreFundar  \n",
      "0         0.584803             AFE  África Oriental y Meridional  \n",
      "1         0.589474             AFE  África Oriental y Meridional  \n",
      "2         0.579662             AFE  África Oriental y Meridional  \n",
      "3         0.568109             AFE  África Oriental y Meridional  \n",
      "4         0.569110             AFE  África Oriental y Meridional  \n",
      "...            ...             ...                           ...  \n",
      "1910      0.731318             WLD                         Mundo  \n",
      "1911      0.724939             WLD                         Mundo  \n",
      "1912      0.717605             WLD                         Mundo  \n",
      "1913      0.711307             WLD                         Mundo  \n",
      "1914      0.705482             WLD                         Mundo  \n",
      "\n",
      "[1915 rows x 7 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'POBREZ'  \n",
    "# ARCHIVO   = 'ISA_global-poverty_it3.csv'\n",
    "# no se registra ni columna similar a geocodigo ni desc_fundar, a pesar de tener una columna llamada region porque el contenido es ‘GBA' y 'national' \n",
    "# que son georreferencial pero no son parte de desc_fundar, y no tiene una columna de geocodigo tampoco\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'POBREZ'  \n",
    "ARCHIVO   = 'ISA_global-poverty_it3.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    anio  mediana_paises_inflacion   pais_o_grupo_de_paises geocodigoFundar  \\\n",
      "0   1992                 17.545874                Argentina            <NA>   \n",
      "1   1993                  7.364981                Argentina            <NA>   \n",
      "2   1994                  3.854381                Argentina            <NA>   \n",
      "3   1995                  1.607754                Argentina            <NA>   \n",
      "4   1996                  0.054311                Argentina            <NA>   \n",
      "..   ...                       ...                      ...             ...   \n",
      "59  2019                  3.531500  América Latina y Caribe            <NA>   \n",
      "60  2020                  3.040500  América Latina y Caribe            <NA>   \n",
      "61  2021                  6.982000  América Latina y Caribe            <NA>   \n",
      "62  2022                  8.375500  América Latina y Caribe            <NA>   \n",
      "63  2023                  4.299000  América Latina y Caribe            <NA>   \n",
      "\n",
      "   geonombreFundar  \n",
      "0             <NA>  \n",
      "1             <NA>  \n",
      "2             <NA>  \n",
      "3             <NA>  \n",
      "4             <NA>  \n",
      "..             ...  \n",
      "59            <NA>  \n",
      "60            <NA>  \n",
      "61            <NA>  \n",
      "62            <NA>  \n",
      "63            <NA>  \n",
      "\n",
      "[64 rows x 5 columns]\n",
      "None\n",
      "    anio  mediana_paises_inflacion   pais_o_grupo_de_paises geocodigoFundar  \\\n",
      "0   1992                 17.545874                Argentina            <NA>   \n",
      "1   1993                  7.364981                Argentina            <NA>   \n",
      "2   1994                  3.854381                Argentina            <NA>   \n",
      "3   1995                  1.607754                Argentina            <NA>   \n",
      "4   1996                  0.054311                Argentina            <NA>   \n",
      "..   ...                       ...                      ...             ...   \n",
      "59  2019                  3.531500  América Latina y Caribe            <NA>   \n",
      "60  2020                  3.040500  América Latina y Caribe            <NA>   \n",
      "61  2021                  6.982000  América Latina y Caribe            <NA>   \n",
      "62  2022                  8.375500  América Latina y Caribe            <NA>   \n",
      "63  2023                  4.299000  América Latina y Caribe            <NA>   \n",
      "\n",
      "   geonombreFundar  \n",
      "0             <NA>  \n",
      "1             <NA>  \n",
      "2             <NA>  \n",
      "3             <NA>  \n",
      "4             <NA>  \n",
      "..             ...  \n",
      "59            <NA>  \n",
      "60            <NA>  \n",
      "61            <NA>  \n",
      "62            <NA>  \n",
      "63            <NA>  \n",
      "\n",
      "[64 rows x 5 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'PRECIO'  \n",
    "# ARCHIVO   = '7_comparacion_inflacion_mediana_argentina_latam_1992_2022.csv'\n",
    "# No se cumple condición X% Los valores de esta columna son: 'Argentina', 'América Latina y Caribe' \n",
    "# 'América Latina y Caribe' no aparece en el geonomenclador, si aparece ‘América Latina y el Caribe’. \n",
    "# No se lee columna similar ni a geocodigo ni a desc_fundar\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'PRECIO'  \n",
    "ARCHIVO   = '7_comparacion_inflacion_mediana_argentina_latam_1992_2022.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              region  tipo_energia  valor_en_mw  porcentaje  \\\n",
      "0   CABA y Provincia de Buenos Aires    Bioenergía           48    3.219316   \n",
      "1   CABA y Provincia de Buenos Aires        Eólica         1443   96.780684   \n",
      "2   CABA y Provincia de Buenos Aires  Fotovoltaica            0    0.000000   \n",
      "3   CABA y Provincia de Buenos Aires         Hidro            0    0.000000   \n",
      "4                             Centro    Bioenergía           34    6.679764   \n",
      "5                             Centro        Eólica          240   47.151277   \n",
      "6                             Centro  Fotovoltaica          118   23.182711   \n",
      "7                             Centro         Hidro          117   22.986248   \n",
      "8                            Comahue    Bioenergía            2    0.680272   \n",
      "9                            Comahue        Eólica          253   86.054422   \n",
      "10                           Comahue  Fotovoltaica            7    2.380952   \n",
      "11                           Comahue         Hidro           32   10.884354   \n",
      "12                              Cuyo    Bioenergía            0    0.000000   \n",
      "13                              Cuyo        Eólica            0    0.000000   \n",
      "14                              Cuyo  Fotovoltaica          564   75.300401   \n",
      "15                              Cuyo         Hidro          185   24.699599   \n",
      "16                           Litoral    Bioenergía           12   85.714286   \n",
      "17                           Litoral        Eólica            0    0.000000   \n",
      "18                           Litoral  Fotovoltaica            0    0.000000   \n",
      "19                           Litoral         Hidro            2   14.285714   \n",
      "20                 Noreste Argentino    Bioenergía          109  100.000000   \n",
      "21                 Noreste Argentino        Eólica            0    0.000000   \n",
      "22                 Noreste Argentino  Fotovoltaica            0    0.000000   \n",
      "23                 Noreste Argentino         Hidro            0    0.000000   \n",
      "24                Noroeste Argentino    Bioenergía           72    6.190886   \n",
      "25                Noroeste Argentino        Eólica          194   16.680997   \n",
      "26                Noroeste Argentino  Fotovoltaica          778   66.895959   \n",
      "27                Noroeste Argentino         Hidro          119   10.232158   \n",
      "28                         Patagonia    Bioenergía            0    0.000000   \n",
      "29                         Patagonia        Eólica         1576   97.104128   \n",
      "30                         Patagonia  Fotovoltaica            0    0.000000   \n",
      "31                         Patagonia         Hidro           47    2.895872   \n",
      "32                             Total    Bioenergía          277    4.653898   \n",
      "33                             Total        Eólica         3706   62.264785   \n",
      "34                             Total  Fotovoltaica         1467   24.647177   \n",
      "35                             Total         Hidro          502    8.434140   \n",
      "\n",
      "   geocodigoFundar geonombreFundar  \n",
      "0             <NA>            <NA>  \n",
      "1             <NA>            <NA>  \n",
      "2             <NA>            <NA>  \n",
      "3             <NA>            <NA>  \n",
      "4             <NA>            <NA>  \n",
      "5             <NA>            <NA>  \n",
      "6             <NA>            <NA>  \n",
      "7             <NA>            <NA>  \n",
      "8             <NA>            <NA>  \n",
      "9             <NA>            <NA>  \n",
      "10            <NA>            <NA>  \n",
      "11            <NA>            <NA>  \n",
      "12            <NA>            <NA>  \n",
      "13            <NA>            <NA>  \n",
      "14            <NA>            <NA>  \n",
      "15            <NA>            <NA>  \n",
      "16            <NA>            <NA>  \n",
      "17            <NA>            <NA>  \n",
      "18            <NA>            <NA>  \n",
      "19            <NA>            <NA>  \n",
      "20            <NA>            <NA>  \n",
      "21            <NA>            <NA>  \n",
      "22            <NA>            <NA>  \n",
      "23            <NA>            <NA>  \n",
      "24            <NA>            <NA>  \n",
      "25            <NA>            <NA>  \n",
      "26            <NA>            <NA>  \n",
      "27            <NA>            <NA>  \n",
      "28            <NA>            <NA>  \n",
      "29            <NA>            <NA>  \n",
      "30            <NA>            <NA>  \n",
      "31            <NA>            <NA>  \n",
      "32            <NA>            <NA>  \n",
      "33            <NA>            <NA>  \n",
      "34            <NA>            <NA>  \n",
      "35            <NA>            <NA>  \n",
      "None\n",
      "                              region  tipo_energia  valor_en_mw  porcentaje  \\\n",
      "0   CABA y Provincia de Buenos Aires    Bioenergía           48    3.219316   \n",
      "1   CABA y Provincia de Buenos Aires        Eólica         1443   96.780684   \n",
      "2   CABA y Provincia de Buenos Aires  Fotovoltaica            0    0.000000   \n",
      "3   CABA y Provincia de Buenos Aires         Hidro            0    0.000000   \n",
      "4                             Centro    Bioenergía           34    6.679764   \n",
      "5                             Centro        Eólica          240   47.151277   \n",
      "6                             Centro  Fotovoltaica          118   23.182711   \n",
      "7                             Centro         Hidro          117   22.986248   \n",
      "8                            Comahue    Bioenergía            2    0.680272   \n",
      "9                            Comahue        Eólica          253   86.054422   \n",
      "10                           Comahue  Fotovoltaica            7    2.380952   \n",
      "11                           Comahue         Hidro           32   10.884354   \n",
      "12                              Cuyo    Bioenergía            0    0.000000   \n",
      "13                              Cuyo        Eólica            0    0.000000   \n",
      "14                              Cuyo  Fotovoltaica          564   75.300401   \n",
      "15                              Cuyo         Hidro          185   24.699599   \n",
      "16                           Litoral    Bioenergía           12   85.714286   \n",
      "17                           Litoral        Eólica            0    0.000000   \n",
      "18                           Litoral  Fotovoltaica            0    0.000000   \n",
      "19                           Litoral         Hidro            2   14.285714   \n",
      "20                 Noreste Argentino    Bioenergía          109  100.000000   \n",
      "21                 Noreste Argentino        Eólica            0    0.000000   \n",
      "22                 Noreste Argentino  Fotovoltaica            0    0.000000   \n",
      "23                 Noreste Argentino         Hidro            0    0.000000   \n",
      "24                Noroeste Argentino    Bioenergía           72    6.190886   \n",
      "25                Noroeste Argentino        Eólica          194   16.680997   \n",
      "26                Noroeste Argentino  Fotovoltaica          778   66.895959   \n",
      "27                Noroeste Argentino         Hidro          119   10.232158   \n",
      "28                         Patagonia    Bioenergía            0    0.000000   \n",
      "29                         Patagonia        Eólica         1576   97.104128   \n",
      "30                         Patagonia  Fotovoltaica            0    0.000000   \n",
      "31                         Patagonia         Hidro           47    2.895872   \n",
      "32                             Total    Bioenergía          277    4.653898   \n",
      "33                             Total        Eólica         3706   62.264785   \n",
      "34                             Total  Fotovoltaica         1467   24.647177   \n",
      "35                             Total         Hidro          502    8.434140   \n",
      "\n",
      "   geocodigoFundar geonombreFundar  \n",
      "0             <NA>            <NA>  \n",
      "1             <NA>            <NA>  \n",
      "2             <NA>            <NA>  \n",
      "3             <NA>            <NA>  \n",
      "4             <NA>            <NA>  \n",
      "5             <NA>            <NA>  \n",
      "6             <NA>            <NA>  \n",
      "7             <NA>            <NA>  \n",
      "8             <NA>            <NA>  \n",
      "9             <NA>            <NA>  \n",
      "10            <NA>            <NA>  \n",
      "11            <NA>            <NA>  \n",
      "12            <NA>            <NA>  \n",
      "13            <NA>            <NA>  \n",
      "14            <NA>            <NA>  \n",
      "15            <NA>            <NA>  \n",
      "16            <NA>            <NA>  \n",
      "17            <NA>            <NA>  \n",
      "18            <NA>            <NA>  \n",
      "19            <NA>            <NA>  \n",
      "20            <NA>            <NA>  \n",
      "21            <NA>            <NA>  \n",
      "22            <NA>            <NA>  \n",
      "23            <NA>            <NA>  \n",
      "24            <NA>            <NA>  \n",
      "25            <NA>            <NA>  \n",
      "26            <NA>            <NA>  \n",
      "27            <NA>            <NA>  \n",
      "28            <NA>            <NA>  \n",
      "29            <NA>            <NA>  \n",
      "30            <NA>            <NA>  \n",
      "31            <NA>            <NA>  \n",
      "32            <NA>            <NA>  \n",
      "33            <NA>            <NA>  \n",
      "34            <NA>            <NA>  \n",
      "35            <NA>            <NA>  \n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'TRANEN'  \n",
    "# ARCHIVO   = 'potencia_instalada_renov_regional.csv'\n",
    "# No se cumple condición X% Los valores de esta columna son: 'CABA y Provincia de Buenos Aires', 'Centro', 'Comahue',\n",
    "# 'Cuyo', 'Litoral', 'Noreste Argentino', 'Noroeste Argentino', 'Patagonia', 'Total'\n",
    "# 'CABA y Provincia de Buenos Aires',  'Noreste Argentino', 'Noroeste Argentino' no aparece en el geonomenclador.\n",
    "# No se lee columna similar ni a geocodigo ni a desc_fundar\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'TRANEN'  \n",
    "ARCHIVO   = 'potencia_instalada_renov_regional.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"bisque\"> LO CORRO PARA TODOS LOS TOPICOS </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TOPICO                                                csv  \\\n",
      "0      SEBACO                      22_participacion_expo_sbc.csv   \n",
      "18     SEBACO                      22_participacion_expo_sbc.csv   \n",
      "36     SEBACO                      23_participacion_expo_ssi.csv   \n",
      "54     SEBACO                        06_empleo_sbc_provincia.csv   \n",
      "81     SEBACO                        06_empleo_sbc_provincia.csv   \n",
      "...       ...                                                ...   \n",
      "33406  ACECON                      A5_pibpc_propeeuu_ppa_usd.csv   \n",
      "33449  INFDES  tasa_informalidad_provincia_definicion_product...   \n",
      "33465  INFDES  tasa_informalidad_provincia_definicion_product...   \n",
      "33481  MINERI             exportaciones_mineras_provinciales.csv   \n",
      "33531  MINERI             exportaciones_mineras_provinciales.csv   \n",
      "\n",
      "                                                problema  \n",
      "0      dataset tiene el codigo 'CHT' que NO esta incl...  \n",
      "18     dataset tiene el codigo 'ROM' que NO esta incl...  \n",
      "36     dataset tiene el codigo 'ROM' que NO esta incl...  \n",
      "54     Para la string 'CABA' se debería generar un co...  \n",
      "81     Para la string 'GBA' se debería generar un cod...  \n",
      "...                                                  ...  \n",
      "33406  dataset tiene el codigo 'SUD', que NO esta inc...  \n",
      "33449  Para la string 'CABA' se debería generar un co...  \n",
      "33465  Para la string 'TOTAL PAIS' se debería generar...  \n",
      "33481  Para la string 'SAN_JUAN' se debería generar u...  \n",
      "33531  Para la string 'SANTA_CRUZ' se debería generar...  \n",
      "\n",
      "[305 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return \"\".join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geonomenclador.iterrows():\n",
    "    raw_code   = str(row['geocodigo'])\n",
    "    code_norm  = sin_tildes(raw_code.strip().upper())\n",
    "    name_long  = row['name_long']\n",
    "    desc_fund  = row['desc_fundar']\n",
    "    geonames_by_geocode[code_norm] = (name_long, desc_fund)\n",
    "\n",
    "# lista problemas por tipo\n",
    "problemas = []\n",
    "\n",
    "# lista de topicos (en este caso es solo pesca)\n",
    "topics = []\n",
    "for d in os.listdir(base_dir):\n",
    "    path = os.path.join(base_dir, d)\n",
    "    if os.path.isdir(path) and d not in ('LICENSE', 'README.md'):\n",
    "        topics.append(d)\n",
    "\n",
    "# proceso cada topico y csv\n",
    "for topico in topics:\n",
    "    src_topic = os.path.join(base_dir, topico)\n",
    "    dst_topic = os.path.join(dest_dir, topico)\n",
    "    os.makedirs(dst_topic, exist_ok = True)\n",
    "    \n",
    "    # listar los archivos csv en este tópico\n",
    "    for archivo in os.listdir(src_topic):\n",
    "        if not archivo.lower().endswith('.csv'):\n",
    "            continue\n",
    "        \n",
    "        # abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "        cfg = df_codificacion[\n",
    "            (df_codificacion['TOPICO'] == topico) &\n",
    "            (df_codificacion['archivo_csv'] == archivo)\n",
    "        ]\n",
    "        if cfg.empty:\n",
    "            print(f\"No hay info encoding/delimiter para {topico}/{archivo}\")\n",
    "            continue\n",
    "        encoding = cfg.iloc[0]['encoding']\n",
    "        delimiter = cfg.iloc[0]['delimiter']\n",
    "        \n",
    "        # del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "        row_cols = columnscsv_Geocodigo_descFundar[\n",
    "            (columnscsv_Geocodigo_descFundar['TOPICO'] == topico) &\n",
    "            (columnscsv_Geocodigo_descFundar['archivo_csv'] == archivo)\n",
    "        ]\n",
    "        col_code = row_cols.iloc[0]['columna_Geocodigo'] if not row_cols.empty else None\n",
    "        col_desc = row_cols.iloc[0]['columna_DescFundar'] if not row_cols.empty else None\n",
    "        \n",
    "        # leo csv\n",
    "        src_path = os.path.join(src_topic, archivo)\n",
    "        df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter)\n",
    "        \n",
    "        # le sumo al csv las nuevas columnas de interes\n",
    "        df['geocodigoFundar'] = pd.NA\n",
    "        df['geonombreFundar'] = pd.NA\n",
    "        \n",
    "        # recorrer filas del csv\n",
    "        for idx in df.index:\n",
    "\n",
    "            # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "            # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "            \n",
    "            if pd.notna(col_code):\n",
    "                raw_code = str(df.at[idx, col_code])\n",
    "                norm_code = sin_tildes(raw_code.strip().upper())\n",
    "                if norm_code in geonames_by_geocode:\n",
    "                    nl, desc0 = geonames_by_geocode[norm_code]\n",
    "                    df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "                else:\n",
    "                    # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "                    if pd.notna(col_desc):\n",
    "                        raw_desc = str(df.at[idx, col_desc])\n",
    "                        norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                        # bsqueda de coincidencias en desc_fundar\n",
    "                        matched_code = []\n",
    "                        for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                            norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                            #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                            if norm_desc0 == norm_desc:\n",
    "                                matched_code.append(code_key)\n",
    "                        if len(matched_code) == 1:\n",
    "                            c0 = matched_code[0]\n",
    "                            nl, desc0 = geonames_by_geocode[c0]\n",
    "                            df.at[idx, 'geocodigoFundar'] = c0\n",
    "                            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                            problemas.append({\n",
    "                                'TOPICO': topico,\n",
    "                                'csv': archivo,\n",
    "                                'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador.\"\n",
    "                            })\n",
    "                        else:\n",
    "                            problemas.append({\n",
    "                                'TOPICO': topico,\n",
    "                                'csv': archivo,\n",
    "                                'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos).\"\n",
    "                            })\n",
    "                    # A.2) no hay columna desc_fundar para intentar\n",
    "\n",
    "                    else:\n",
    "                        problemas.append({\n",
    "                            'TOPICO': topico,\n",
    "                            'csv': archivo,\n",
    "                            'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos).\"\n",
    "                        })\n",
    "            \n",
    "            # CASO B: no existe columna de geocodigo, pero si se tiene de desc_fundar\n",
    "\n",
    "            elif pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "                    if sin_tildes(str(desc0).strip().upper()) == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                else:\n",
    "                    problemas.append({\n",
    "                        'TOPICO': topico,\n",
    "                        'csv': archivo,\n",
    "                        'problema': f\"Para la string '{norm_desc}' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos).\"\n",
    "                    })\n",
    "            \n",
    "            # CASO C: si no hay columnas asociadas, quedan NaN\n",
    "        \n",
    "        # guardar csv copia modificado en su correspondiente topico, con UTF-8 y separador coma\n",
    "        dst_path = os.path.join(dest_dir, topico, archivo)\n",
    "        df.to_csv(dst_path, index = False,  encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# df de problemas\n",
    "df_problemas = pd.DataFrame(problemas) # eliminar duplicados de \"problemas\" que se repiten para TOPICO | csv\n",
    "df_problemas = df_problemas.drop_duplicates(subset=['TOPICO', 'csv', 'problema'])\n",
    "df_problemas.to_csv('/home/lucia/Desktop/Fundar/Argendata_renaming/data/df_problemas.csv', index = False,  encoding = 'utf-8', sep = ',')\n",
    "print(df_problemas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"bisque\"> CLASIFICACION PROBLEMAS </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba de clasificacion problemas\n",
    "\n",
    "# 1. codigo no en geocodigos pero desc_fundar coincide\n",
    "mask1 = (\n",
    "    df_problemas['problema'].str.contains(\n",
    "        \"que NO esta incluido en los geocodigos pero tiene la string\"\n",
    "    ) &\n",
    "    df_problemas['problema'].str.contains(\"No hay que modificar geonomenclador\")\n",
    ")\n",
    "df_tipo1 = df_problemas[mask1]\n",
    "df_tipo1.to_csv('/home/lucia/Desktop/Fundar/Argendata_renaming/data/df_tipo1.csv', index = False,  encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# 2. codigo no en geocodigos y desc_fundar no coincide\n",
    "mask2 = df_problemas['problema'].str.contains(\n",
    "    \"que NO esta incluido en los geocodigos y tiene la string\"\n",
    ") & df_problemas['problema'].str.contains(\n",
    "    \"que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones\"\n",
    ")\n",
    "df_tipo2 = df_problemas[mask2]\n",
    "df_tipo2.to_csv('/home/lucia/Desktop/Fundar/Argendata_renaming/data/df_tipo2.csv', index = False,  encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# 3. codigo no en geocodigos y no existe columna desc_fundar\n",
    "mask3 = df_problemas['problema'].str.contains(\n",
    "    \"que NO esta incluido en los geocodigos y no existe columna desc_fundar\"\n",
    ")\n",
    "df_tipo3 = df_problemas[mask3]\n",
    "df_tipo3.to_csv('/home/lucia/Desktop/Fundar/Argendata_renaming/data/df_tipo3.csv', index = False,  encoding = 'utf-8', sep = ',')\n",
    "\n",
    "# 4. Para la string ... se debería generar un codigo nuevo\n",
    "mask4 = df_problemas['problema'].str.contains(\n",
    "    \"Para la string '.*' se debería generar un codigo nuevo\",\n",
    "    regex=True\n",
    ")\n",
    "df_tipo4 = df_problemas[mask4]\n",
    "df_tipo4.to_csv('/home/lucia/Desktop/Fundar/Argendata_renaming/data/df_tipo4.csv', index = False,  encoding = 'utf-8', sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOPICO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "problema",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4fb52688-ac78-458e-a8c0-fad3bde486ee",
       "rows": [
        [
         "58",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'KOS', que NO esta incluido en los geocodigos pero tiene la string 'KOSOVO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "65",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'SD1', que NO esta incluido en los geocodigos pero tiene la string 'SUDAN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "72",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'SV1', que NO esta incluido en los geocodigos pero tiene la string 'UNION SOVIETICA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "86",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'TZ2', que NO esta incluido en los geocodigos pero tiene la string 'ZANZIBAR' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "93",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'YM1', que NO esta incluido en los geocodigos pero tiene la string 'REPUBLICA ARABICA DE YEMEN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "100",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'YUGOSLAVIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2655",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'A5_A7', que NO esta incluido en los geocodigos pero tiene la string 'AMERICA DEL SUR Y CENTRAL' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2681",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'E', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2733",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'F', que NO esta incluido en los geocodigos pero tiene la string 'AFRICA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2785",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'S2', que NO esta incluido en los geocodigos pero tiene la string 'ASIA DEL ESTE' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2837",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'W', que NO esta incluido en los geocodigos pero tiene la string 'MUNDO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2863",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'WXD', que NO esta incluido en los geocodigos pero tiene la string 'RESTO DEL MUNDO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2889",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'WXOECD', que NO esta incluido en los geocodigos pero tiene la string 'ECONOMIAS NO MIEMBROS DE OCDE' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2915",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'W_O', que NO esta incluido en los geocodigos pero tiene la string 'OTRAS REGIONES' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2958",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'KOS', que NO esta incluido en los geocodigos pero tiene la string 'KOSOVO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2959",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'SD1', que NO esta incluido en los geocodigos pero tiene la string 'SUDAN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2960",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'SV1', que NO esta incluido en los geocodigos pero tiene la string 'UNION SOVIETICA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2962",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'TZ2', que NO esta incluido en los geocodigos pero tiene la string 'ZANZIBAR' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2963",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'YM1', que NO esta incluido en los geocodigos pero tiene la string 'REPUBLICA ARABICA DE YEMEN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "2964",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'YUGOSLAVIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3372",
         "ACECON",
         "A5_pibpc_propeeuu_ppa_usd.csv",
         "dataset tiene el codigo 'ROM', que NO esta incluido en los geocodigos pero tiene la string 'RUMANIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3415",
         "ACECON",
         "A5_pibpc_propeeuu_ppa_usd.csv",
         "dataset tiene el codigo 'SUD', que NO esta incluido en los geocodigos pero tiene la string 'SUDAN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3458",
         "ACECON",
         "2_pibpc_salud_edu.csv",
         "dataset tiene el codigo 'ROM', que NO esta incluido en los geocodigos pero tiene la string 'RUMANIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3459",
         "ACECON",
         "2_pibpc_salud_edu.csv",
         "dataset tiene el codigo 'SUD', que NO esta incluido en los geocodigos pero tiene la string 'SUDAN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3534",
         "ACECON",
         "A1_inb_pib.csv",
         "dataset tiene el codigo 'ROM', que NO esta incluido en los geocodigos pero tiene la string 'RUMANIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3597",
         "ACECON",
         "A1_inb_pib.csv",
         "dataset tiene el codigo 'SUD', que NO esta incluido en los geocodigos pero tiene la string 'SUDAN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3724",
         "ACECON",
         "A4_efecto_bs.csv",
         "dataset tiene el codigo 'ROM', que NO esta incluido en los geocodigos pero tiene la string 'RUMANIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3725",
         "ACECON",
         "A4_efecto_bs.csv",
         "dataset tiene el codigo 'SUD', que NO esta incluido en los geocodigos pero tiene la string 'SUDAN' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3951",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'AFRICA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3952",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3954",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'ASIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "3955",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'OCEANIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "4159",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'AFRICA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "4160",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'OCEANIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "4162",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "4163",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG', que NO esta incluido en los geocodigos pero tiene la string 'ASIA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "8745",
         "CAMCLI",
         "emisiones_anuales_co2_region.csv",
         "dataset tiene el codigo 'OWID_KOS', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "14452",
         "CAMCLI",
         "emisiones_anuales_co2_region_2021.csv",
         "dataset tiene el codigo 'OWID_KOS', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "24631",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZF.EAP', que NO esta incluido en los geocodigos pero tiene la string 'ESTE DE ASIA Y PACIFICO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "24664",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZG.ECA', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA Y ASIA CENTRAL' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "24697",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZH.LAC', que NO esta incluido en los geocodigos pero tiene la string 'AMERICA LATINA Y EL CARIBE' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "24796",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZK.WORLD', que NO esta incluido en los geocodigos pero tiene la string 'MUNDO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25137",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZF.EAP', que NO esta incluido en los geocodigos pero tiene la string 'ESTE DE ASIA Y PACIFICO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25170",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZG.ECA', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA Y ASIA CENTRAL' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25203",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZH.LAC', que NO esta incluido en los geocodigos pero tiene la string 'AMERICA LATINA Y EL CARIBE' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25302",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZK.WORLD', que NO esta incluido en los geocodigos pero tiene la string 'MUNDO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25500",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZF.EAP', que NO esta incluido en los geocodigos pero tiene la string 'ESTE DE ASIA Y PACIFICO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25533",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZG.ECA', que NO esta incluido en los geocodigos pero tiene la string 'EUROPA Y ASIA CENTRAL' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25566",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZH.LAC', que NO esta incluido en los geocodigos pero tiene la string 'AMERICA LATINA Y EL CARIBE' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ],
        [
         "25665",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZK.WORLD', que NO esta incluido en los geocodigos pero tiene la string 'MUNDO' que matchea sin ambiguedad en desc_fundar. No hay que modificar geonomenclador."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 113
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICO</th>\n",
       "      <th>csv</th>\n",
       "      <th>problema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais.csv</td>\n",
       "      <td>dataset tiene el codigo 'KOS', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais.csv</td>\n",
       "      <td>dataset tiene el codigo 'SD1', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais.csv</td>\n",
       "      <td>dataset tiene el codigo 'SV1', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais.csv</td>\n",
       "      <td>dataset tiene el codigo 'TZ2', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais.csv</td>\n",
       "      <td>dataset tiene el codigo 'YM1', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33198</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>produccion_global_carne_bovina_evo.csv</td>\n",
       "      <td>dataset tiene el codigo 'F51', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33230</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>rindes_maiz_internacional.csv</td>\n",
       "      <td>dataset tiene el codigo 'F15', que NO esta inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33269</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>rindes_maiz_internacional.csv</td>\n",
       "      <td>dataset tiene el codigo 'F228', que NO esta in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33300</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>rindes_maiz_internacional.csv</td>\n",
       "      <td>dataset tiene el codigo 'F248', que NO esta in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33331</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>rindes_maiz_internacional.csv</td>\n",
       "      <td>dataset tiene el codigo 'F51', que NO esta inc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOPICO                                     csv  \\\n",
       "58     ESTPRO                   va_sectorial_pais.csv   \n",
       "65     ESTPRO                   va_sectorial_pais.csv   \n",
       "72     ESTPRO                   va_sectorial_pais.csv   \n",
       "86     ESTPRO                   va_sectorial_pais.csv   \n",
       "93     ESTPRO                   va_sectorial_pais.csv   \n",
       "...       ...                                     ...   \n",
       "33198  AGROPE  produccion_global_carne_bovina_evo.csv   \n",
       "33230  AGROPE           rindes_maiz_internacional.csv   \n",
       "33269  AGROPE           rindes_maiz_internacional.csv   \n",
       "33300  AGROPE           rindes_maiz_internacional.csv   \n",
       "33331  AGROPE           rindes_maiz_internacional.csv   \n",
       "\n",
       "                                                problema  \n",
       "58     dataset tiene el codigo 'KOS', que NO esta inc...  \n",
       "65     dataset tiene el codigo 'SD1', que NO esta inc...  \n",
       "72     dataset tiene el codigo 'SV1', que NO esta inc...  \n",
       "86     dataset tiene el codigo 'TZ2', que NO esta inc...  \n",
       "93     dataset tiene el codigo 'YM1', que NO esta inc...  \n",
       "...                                                  ...  \n",
       "33198  dataset tiene el codigo 'F51', que NO esta inc...  \n",
       "33230  dataset tiene el codigo 'F15', que NO esta inc...  \n",
       "33269  dataset tiene el codigo 'F228', que NO esta in...  \n",
       "33300  dataset tiene el codigo 'F248', que NO esta in...  \n",
       "33331  dataset tiene el codigo 'F51', que NO esta inc...  \n",
       "\n",
       "[113 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tipo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOPICO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "problema",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "849f50e8-a223-4a0c-b8a6-6fe45dc9a9ae",
       "rows": [
        [
         "79",
         "ESTPRO",
         "va_sectorial_pais.csv",
         "dataset tiene el codigo 'TZ1' que NO esta incluido en los geocodigos y tiene la string 'TANZANIA (CONTINENTAL)' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "2707",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'EU28XEU15' que NO esta incluido en los geocodigos y tiene la string 'UNION EUROPEA (28 PAISES EXCELUYENDO EU15)' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "2759",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'NAFTA' que NO esta incluido en los geocodigos y tiene la string 'PAISES NAFTA' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "2811",
         "ESTPRO",
         "particip_va_intensivos_id_bys.csv",
         "dataset tiene el codigo 'S2_S8' que NO esta incluido en los geocodigos y tiene la string 'ASIA DEL ESTE Y SUDESTE' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "2961",
         "ESTPRO",
         "va_sectorial_pais_bys.csv",
         "dataset tiene el codigo 'TZ1' que NO esta incluido en los geocodigos y tiene la string 'TANZANIA (CONTINENTAL)' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3329",
         "ACECON",
         "A5_pibpc_propeeuu_ppa_usd.csv",
         "dataset tiene el codigo 'ZAR' que NO esta incluido en los geocodigos y tiene la string 'RD CONGO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3460",
         "ACECON",
         "2_pibpc_salud_edu.csv",
         "dataset tiene el codigo 'ZAR' que NO esta incluido en los geocodigos y tiene la string 'RD CONGO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3660",
         "ACECON",
         "A1_inb_pib.csv",
         "dataset tiene el codigo 'TMP' que NO esta incluido en los geocodigos y tiene la string 'TIMOR ORIENTAL' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3723",
         "ACECON",
         "A4_efecto_bs.csv",
         "dataset tiene el codigo 'ZAR' que NO esta incluido en los geocodigos y tiene la string 'RD CONGO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3950",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y tiene la string 'AMERICA DEL SUR' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3953",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y tiene la string 'AMERICA DEL NORTE' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3961",
         "COMEXT",
         "composicion_exportaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y tiene la string 'OTROS' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4161",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y tiene la string 'OTROS' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4164",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y tiene la string 'AMERICA DEL SUR' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4165",
         "COMEXT",
         "composicion_importaciones_bienes_regiones_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y tiene la string 'AMERICA DEL NORTE' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "5088",
         "COMEXT",
         "exportaciones_servicios_top_20_destinos.csv",
         "dataset tiene el codigo 'NAN' que NO esta incluido en los geocodigos y tiene la string 'RESTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "5196",
         "CAMCLI",
         "emisiones_anuales_co2_region.csv",
         "dataset tiene el codigo 'NAN' que NO esta incluido en los geocodigos y tiene la string 'NAN' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "13932",
         "CAMCLI",
         "emisiones_anuales_co2_region.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y tiene la string 'NAN' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24466",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZA.VHHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MUY ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24499",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZB.HHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24532",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZC.MHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MEDIO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24565",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZD.LHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO BAJO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24598",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZE.AS' que NO esta incluido en los geocodigos y tiene la string 'ESTADOS ARABES' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24730",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZI.SA' que NO esta incluido en los geocodigos y tiene la string 'ASIA MERIDIONAL' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24763",
         "DESHUM",
         "expectativa_educ.csv",
         "dataset tiene el codigo 'ZZJ.SSA' que NO esta incluido en los geocodigos y tiene la string 'AFRICA SUBSAHARIANA' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24972",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZA.VHHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MUY ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25005",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZB.HHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25038",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZC.MHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MEDIO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25071",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZD.LHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO BAJO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25104",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZE.AS' que NO esta incluido en los geocodigos y tiene la string 'ESTADOS ARABES' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25236",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZI.SA' que NO esta incluido en los geocodigos y tiene la string 'ASIA MERIDIONAL' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25269",
         "DESHUM",
         "indice_desarrollo_humano.csv",
         "dataset tiene el codigo 'ZZJ.SSA' que NO esta incluido en los geocodigos y tiene la string 'AFRICA SUBSAHARIANA' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25335",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZA.VHHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MUY ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25368",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZB.HHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25401",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZC.MHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MEDIO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25434",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZD.LHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO BAJO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25467",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZE.AS' que NO esta incluido en los geocodigos y tiene la string 'ESTADOS ARABES' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25599",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZI.SA' que NO esta incluido en los geocodigos y tiene la string 'ASIA MERIDIONAL' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25632",
         "DESHUM",
         "anios_prom_educ.csv",
         "dataset tiene el codigo 'ZZJ.SSA' que NO esta incluido en los geocodigos y tiene la string 'AFRICA SUBSAHARIANA' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28515",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZA.VHHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MUY ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28548",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZB.HHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28581",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZC.MHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MEDIO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28614",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZD.LHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO BAJO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28647",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZE.AS' que NO esta incluido en los geocodigos y tiene la string 'ESTADOS ARABES' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28779",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZI.SA' que NO esta incluido en los geocodigos y tiene la string 'ASIA MERIDIONAL' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28812",
         "DESHUM",
         "expectativa_vida.csv",
         "dataset tiene el codigo 'ZZJ.SSA' que NO esta incluido en los geocodigos y tiene la string 'AFRICA SUBSAHARIANA' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28878",
         "DESHUM",
         "idh_dif_idhp.csv",
         "dataset tiene el codigo 'ZZA.VHHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MUY ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28911",
         "DESHUM",
         "idh_dif_idhp.csv",
         "dataset tiene el codigo 'ZZB.HHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO ALTO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28944",
         "DESHUM",
         "idh_dif_idhp.csv",
         "dataset tiene el codigo 'ZZC.MHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO MEDIO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "28977",
         "DESHUM",
         "idh_dif_idhp.csv",
         "dataset tiene el codigo 'ZZD.LHD' que NO esta incluido en los geocodigos y tiene la string 'DESARROLLO HUMANO BAJO' que NO matchea sin ambiguedad en desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 68
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICO</th>\n",
       "      <th>csv</th>\n",
       "      <th>problema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais.csv</td>\n",
       "      <td>dataset tiene el codigo 'TZ1' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>particip_va_intensivos_id_bys.csv</td>\n",
       "      <td>dataset tiene el codigo 'EU28XEU15' que NO est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>particip_va_intensivos_id_bys.csv</td>\n",
       "      <td>dataset tiene el codigo 'NAFTA' que NO esta in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>particip_va_intensivos_id_bys.csv</td>\n",
       "      <td>dataset tiene el codigo 'S2_S8' que NO esta in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>va_sectorial_pais_bys.csv</td>\n",
       "      <td>dataset tiene el codigo 'TZ1' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32385</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>inb_pc.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZD.LHD' que NO esta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32418</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>inb_pc.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZE.AS' que NO esta i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32550</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>inb_pc.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZI.SA' que NO esta i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32583</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>inb_pc.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZJ.SSA' que NO esta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32963</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>produccion_acuicultura_global_2021.csv</td>\n",
       "      <td>dataset tiene el codigo 'EAZ' que NO esta incl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOPICO                                     csv  \\\n",
       "79     ESTPRO                   va_sectorial_pais.csv   \n",
       "2707   ESTPRO       particip_va_intensivos_id_bys.csv   \n",
       "2759   ESTPRO       particip_va_intensivos_id_bys.csv   \n",
       "2811   ESTPRO       particip_va_intensivos_id_bys.csv   \n",
       "2961   ESTPRO               va_sectorial_pais_bys.csv   \n",
       "...       ...                                     ...   \n",
       "32385  DESHUM                              inb_pc.csv   \n",
       "32418  DESHUM                              inb_pc.csv   \n",
       "32550  DESHUM                              inb_pc.csv   \n",
       "32583  DESHUM                              inb_pc.csv   \n",
       "32963  AGROPE  produccion_acuicultura_global_2021.csv   \n",
       "\n",
       "                                                problema  \n",
       "79     dataset tiene el codigo 'TZ1' que NO esta incl...  \n",
       "2707   dataset tiene el codigo 'EU28XEU15' que NO est...  \n",
       "2759   dataset tiene el codigo 'NAFTA' que NO esta in...  \n",
       "2811   dataset tiene el codigo 'S2_S8' que NO esta in...  \n",
       "2961   dataset tiene el codigo 'TZ1' que NO esta incl...  \n",
       "...                                                  ...  \n",
       "32385  dataset tiene el codigo 'ZZD.LHD' que NO esta ...  \n",
       "32418  dataset tiene el codigo 'ZZE.AS' que NO esta i...  \n",
       "32550  dataset tiene el codigo 'ZZI.SA' que NO esta i...  \n",
       "32583  dataset tiene el codigo 'ZZJ.SSA' que NO esta ...  \n",
       "32963  dataset tiene el codigo 'EAZ' que NO esta incl...  \n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tipo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOPICO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "problema",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f8b5a78c-f867-4ab8-94ee-30cbe5f036a9",
       "rows": [
        [
         "0",
         "SALING",
         "ISA_salarios_mundo_i1.csv",
         "dataset tiene el codigo 'ELS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "1",
         "SALING",
         "ISA_salarios_mundo_i1.csv",
         "dataset tiene el codigo 'PAR' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3780",
         "SEBACO",
         "22_participacion_expo_sbc.csv",
         "dataset tiene el codigo 'CHT' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3798",
         "SEBACO",
         "22_participacion_expo_sbc.csv",
         "dataset tiene el codigo 'ROM' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3816",
         "SEBACO",
         "23_participacion_expo_ssi.csv",
         "dataset tiene el codigo 'ROM' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3834",
         "COMEXT",
         "composicion_importaciones_bienes_sectores_Brambilla_Porto.csv",
         "dataset tiene el codigo 'S19' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3847",
         "COMEXT",
         "cambio_destinos_exportacion.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4146",
         "COMEXT",
         "composicion_exportaciones_bienes_sectores_Brambilla_Porto.csv",
         "dataset tiene el codigo 'S19' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4355",
         "COMEXT",
         "participacion_exportaciones_bienes_servicios_mundo.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4385",
         "COMEXT",
         "composicion_exportaciones_bienes_sitc_seccion.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4685",
         "COMEXT",
         "composicion_importaciones_bienes_sitc_seccion.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "4985",
         "COMEXT",
         "cambio_origenes_importacion.csv",
         "dataset tiene el codigo 'YUG' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14205",
         "CAMCLI",
         "emisiones_per_cap.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14224",
         "CAMCLI",
         "emisiones_per_cap.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14453",
         "TRANEN",
         "capacidad_instalada_eolica_gw.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14476",
         "TRANEN",
         "capacidad_instalada_eolica_gw.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14499",
         "TRANEN",
         "capacidad_instalada_fv_gw.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14522",
         "TRANEN",
         "capacidad_instalada_fv_gw.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14545",
         "TRANEN",
         "generacion_hidro_twh.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14604",
         "TRANEN",
         "generacion_hidro_twh.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14663",
         "TRANEN",
         "generacion_nuclear_twh.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14722",
         "TRANEN",
         "generacion_nuclear_twh.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "14781",
         "TRANEN",
         "matriz_prim_mundo_historic.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "15371",
         "TRANEN",
         "energia_baja_carbono.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "15430",
         "TRANEN",
         "intensidad_carbono_electri_mundo.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "15464",
         "TRANEN",
         "intensidad_carbono_electri_mundo.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "15498",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "16296",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "17094",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_CZS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "17892",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_YGS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "18690",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_ERE' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "19488",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_GFR' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "20286",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_GDR' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "21084",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_SRM' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "21882",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_YAR' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "22680",
         "TRANEN",
         "identidad_kaya_mundo.csv",
         "dataset tiene el codigo 'OWID_YPR' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "23478",
         "TRANEN",
         "intensidad_energ_mundo.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "23536",
         "TRANEN",
         "intensidad_energ_mundo.csv",
         "dataset tiene el codigo 'OWID_CZS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "23594",
         "TRANEN",
         "intensidad_energ_mundo.csv",
         "dataset tiene el codigo 'OWID_YGS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "23652",
         "TRANEN",
         "produccion_biocomb.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "23686",
         "TRANEN",
         "produc_electricidad_fuente_mundo_twh.csv",
         "dataset tiene el codigo 'OWID_WRL' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "23725",
         "TRANEN",
         "produc_electricidad_fuente_mundo_twh.csv",
         "dataset tiene el codigo 'OWID_KOS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24829",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZA.VHHD' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24842",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZB.HHD' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24855",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZC.MHD' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24868",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZD.LHD' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24881",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZE.AS' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24894",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZF.EAP' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24907",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZG.ECA' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "24920",
         "DESHUM",
         "idh_idhd.csv",
         "dataset tiene el codigo 'ZZH.LAC' que NO esta incluido en los geocodigos y no existe columna desc_fundar. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 97
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICO</th>\n",
       "      <th>csv</th>\n",
       "      <th>problema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_salarios_mundo_i1.csv</td>\n",
       "      <td>dataset tiene el codigo 'ELS' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_salarios_mundo_i1.csv</td>\n",
       "      <td>dataset tiene el codigo 'PAR' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>22_participacion_expo_sbc.csv</td>\n",
       "      <td>dataset tiene el codigo 'CHT' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>22_participacion_expo_sbc.csv</td>\n",
       "      <td>dataset tiene el codigo 'ROM' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3816</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>23_participacion_expo_ssi.csv</td>\n",
       "      <td>dataset tiene el codigo 'ROM' que NO esta incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31153</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>idh_subcomponentes.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZG.ECA' que NO esta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>idh_subcomponentes.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZH.LAC' que NO esta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31549</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>idh_subcomponentes.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZI.SA' que NO esta i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31747</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>idh_subcomponentes.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZJ.SSA' que NO esta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31945</th>\n",
       "      <td>DESHUM</td>\n",
       "      <td>idh_subcomponentes.csv</td>\n",
       "      <td>dataset tiene el codigo 'ZZK.WORLD' que NO est...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOPICO                            csv  \\\n",
       "0      SALING      ISA_salarios_mundo_i1.csv   \n",
       "1      SALING      ISA_salarios_mundo_i1.csv   \n",
       "3780   SEBACO  22_participacion_expo_sbc.csv   \n",
       "3798   SEBACO  22_participacion_expo_sbc.csv   \n",
       "3816   SEBACO  23_participacion_expo_ssi.csv   \n",
       "...       ...                            ...   \n",
       "31153  DESHUM         idh_subcomponentes.csv   \n",
       "31351  DESHUM         idh_subcomponentes.csv   \n",
       "31549  DESHUM         idh_subcomponentes.csv   \n",
       "31747  DESHUM         idh_subcomponentes.csv   \n",
       "31945  DESHUM         idh_subcomponentes.csv   \n",
       "\n",
       "                                                problema  \n",
       "0      dataset tiene el codigo 'ELS' que NO esta incl...  \n",
       "1      dataset tiene el codigo 'PAR' que NO esta incl...  \n",
       "3780   dataset tiene el codigo 'CHT' que NO esta incl...  \n",
       "3798   dataset tiene el codigo 'ROM' que NO esta incl...  \n",
       "3816   dataset tiene el codigo 'ROM' que NO esta incl...  \n",
       "...                                                  ...  \n",
       "31153  dataset tiene el codigo 'ZZG.ECA' que NO esta ...  \n",
       "31351  dataset tiene el codigo 'ZZH.LAC' que NO esta ...  \n",
       "31549  dataset tiene el codigo 'ZZI.SA' que NO esta i...  \n",
       "31747  dataset tiene el codigo 'ZZJ.SSA' que NO esta ...  \n",
       "31945  dataset tiene el codigo 'ZZK.WORLD' que NO est...  \n",
       "\n",
       "[97 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tipo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOPICO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "problema",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c934c691-e73d-4121-9bc7-f6777028f49a",
       "rows": [
        [
         "2",
         "SALING",
         "ISA_tipo_empleo_i3.csv",
         "Para la string 'REP. DOMINICANA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "9",
         "INFDES",
         "tasa_informalidad_provincia_definicion_productiva_legal.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "25",
         "INFDES",
         "tasa_informalidad_provincia_definicion_productiva_legal.csv",
         "Para la string 'TOTAL PAIS' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "41",
         "ESTPRO",
         "densidad_empresarial_depto.csv",
         "Para la string 'CIUDAD DE BUENOS AIRES' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "55",
         "ESTPRO",
         "densidad_empresarial_depto.csv",
         "Para la string 'NAN' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "2941",
         "ESTPRO",
         "densidad_nbi.csv",
         "Para la string 'CIUDAD DE BUENOS AIRES' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "2955",
         "ESTPRO",
         "densidad_nbi.csv",
         "Para la string 'NAN' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3461",
         "ACECON",
         "9_pibpc_ppa_log_1950.csv",
         "Para la string 'TIGRES ASIATICOS' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3726",
         "SEBACO",
         "06_empleo_sbc_provincia.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "3753",
         "SEBACO",
         "06_empleo_sbc_provincia.csv",
         "Para la string 'GBA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "5096",
         "MINERI",
         "exportaciones_mineras_provinciales.csv",
         "Para la string 'SAN_JUAN' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "5146",
         "MINERI",
         "exportaciones_mineras_provinciales.csv",
         "Para la string 'SANTA_CRUZ' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "32782",
         "AGROPE",
         "PBG_participacion_agro_provincia_evolucion.csv",
         "Para la string 'CIUDAD DE BUENOS AIRES' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33363",
         "AGROPE",
         "stock_bovina_regiones_argentina.csv",
         "Para la string 'NEA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33364",
         "AGROPE",
         "stock_bovina_regiones_argentina.csv",
         "Para la string 'NOA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33395",
         "MERTRA",
         "tasa_empleo_anio_provincia_sexo.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33398",
         "MERTRA",
         "tasa_empleo_anio_provincia_sexo.csv",
         "Para la string 'TODAS' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33443",
         "MERTRA",
         "tasa_empleo_menores_provincia.csv",
         "Para la string 'CIUDAD DE BUENOS AIRES' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33444",
         "MERTRA",
         "tasa_empleo_menores_provincia.csv",
         "Para la string 'TOTAL' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33459",
         "MERTRA",
         "tasa_empleo_anio_provincia.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33467",
         "MERTRA",
         "tasa_empleo_anio_provincia.csv",
         "Para la string 'TOTAL PAIS' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33475",
         "MERTRA",
         "establecimientos_tasa_empleo_provincia.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33476",
         "MERTRA",
         "lavarropas_tasa_empleo_fem_provincia.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33477",
         "MERTRA",
         "tasa_empleo_mujer_salario_provincia_anio.csv",
         "Para la string 'CIUDAD DE BUENOS AIRES' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33485",
         "MERTRA",
         "tasa_empleo_provincia_nivel_educativo.csv",
         "Para la string 'CIUDAD DE BUENOS AIRES' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33517",
         "MERTRA",
         "tasa_empleo_por_franja_etaria_anio_provincia.csv",
         "Para la string 'CABA' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ],
        [
         "33521",
         "MERTRA",
         "tasa_empleo_por_franja_etaria_anio_provincia.csv",
         "Para la string 'TOTAL' se debería generar un codigo nuevo. Puede implicar modificaciones del geonomenclador (a revisión, para no pisarse con otros casos)."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICO</th>\n",
       "      <th>csv</th>\n",
       "      <th>problema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_tipo_empleo_i3.csv</td>\n",
       "      <td>Para la string 'REP. DOMINICANA' se debería ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INFDES</td>\n",
       "      <td>tasa_informalidad_provincia_definicion_product...</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>INFDES</td>\n",
       "      <td>tasa_informalidad_provincia_definicion_product...</td>\n",
       "      <td>Para la string 'TOTAL PAIS' se debería generar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>densidad_empresarial_depto.csv</td>\n",
       "      <td>Para la string 'CIUDAD DE BUENOS AIRES' se deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>densidad_empresarial_depto.csv</td>\n",
       "      <td>Para la string 'NAN' se debería generar un cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>densidad_nbi.csv</td>\n",
       "      <td>Para la string 'CIUDAD DE BUENOS AIRES' se deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>ESTPRO</td>\n",
       "      <td>densidad_nbi.csv</td>\n",
       "      <td>Para la string 'NAN' se debería generar un cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>ACECON</td>\n",
       "      <td>9_pibpc_ppa_log_1950.csv</td>\n",
       "      <td>Para la string 'TIGRES ASIATICOS' se debería g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>06_empleo_sbc_provincia.csv</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>SEBACO</td>\n",
       "      <td>06_empleo_sbc_provincia.csv</td>\n",
       "      <td>Para la string 'GBA' se debería generar un cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>exportaciones_mineras_provinciales.csv</td>\n",
       "      <td>Para la string 'SAN_JUAN' se debería generar u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>MINERI</td>\n",
       "      <td>exportaciones_mineras_provinciales.csv</td>\n",
       "      <td>Para la string 'SANTA_CRUZ' se debería generar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32782</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>PBG_participacion_agro_provincia_evolucion.csv</td>\n",
       "      <td>Para la string 'CIUDAD DE BUENOS AIRES' se deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33363</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>stock_bovina_regiones_argentina.csv</td>\n",
       "      <td>Para la string 'NEA' se debería generar un cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33364</th>\n",
       "      <td>AGROPE</td>\n",
       "      <td>stock_bovina_regiones_argentina.csv</td>\n",
       "      <td>Para la string 'NOA' se debería generar un cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33395</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_anio_provincia_sexo.csv</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33398</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_anio_provincia_sexo.csv</td>\n",
       "      <td>Para la string 'TODAS' se debería generar un c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33443</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_menores_provincia.csv</td>\n",
       "      <td>Para la string 'CIUDAD DE BUENOS AIRES' se deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33444</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_menores_provincia.csv</td>\n",
       "      <td>Para la string 'TOTAL' se debería generar un c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33459</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_anio_provincia.csv</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33467</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_anio_provincia.csv</td>\n",
       "      <td>Para la string 'TOTAL PAIS' se debería generar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33475</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>establecimientos_tasa_empleo_provincia.csv</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33476</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>lavarropas_tasa_empleo_fem_provincia.csv</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33477</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_mujer_salario_provincia_anio.csv</td>\n",
       "      <td>Para la string 'CIUDAD DE BUENOS AIRES' se deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33485</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_provincia_nivel_educativo.csv</td>\n",
       "      <td>Para la string 'CIUDAD DE BUENOS AIRES' se deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33517</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_por_franja_etaria_anio_provincia.csv</td>\n",
       "      <td>Para la string 'CABA' se debería generar un co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33521</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_por_franja_etaria_anio_provincia.csv</td>\n",
       "      <td>Para la string 'TOTAL' se debería generar un c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOPICO                                                csv  \\\n",
       "2      SALING                             ISA_tipo_empleo_i3.csv   \n",
       "9      INFDES  tasa_informalidad_provincia_definicion_product...   \n",
       "25     INFDES  tasa_informalidad_provincia_definicion_product...   \n",
       "41     ESTPRO                     densidad_empresarial_depto.csv   \n",
       "55     ESTPRO                     densidad_empresarial_depto.csv   \n",
       "2941   ESTPRO                                   densidad_nbi.csv   \n",
       "2955   ESTPRO                                   densidad_nbi.csv   \n",
       "3461   ACECON                           9_pibpc_ppa_log_1950.csv   \n",
       "3726   SEBACO                        06_empleo_sbc_provincia.csv   \n",
       "3753   SEBACO                        06_empleo_sbc_provincia.csv   \n",
       "5096   MINERI             exportaciones_mineras_provinciales.csv   \n",
       "5146   MINERI             exportaciones_mineras_provinciales.csv   \n",
       "32782  AGROPE     PBG_participacion_agro_provincia_evolucion.csv   \n",
       "33363  AGROPE                stock_bovina_regiones_argentina.csv   \n",
       "33364  AGROPE                stock_bovina_regiones_argentina.csv   \n",
       "33395  MERTRA                tasa_empleo_anio_provincia_sexo.csv   \n",
       "33398  MERTRA                tasa_empleo_anio_provincia_sexo.csv   \n",
       "33443  MERTRA                  tasa_empleo_menores_provincia.csv   \n",
       "33444  MERTRA                  tasa_empleo_menores_provincia.csv   \n",
       "33459  MERTRA                     tasa_empleo_anio_provincia.csv   \n",
       "33467  MERTRA                     tasa_empleo_anio_provincia.csv   \n",
       "33475  MERTRA         establecimientos_tasa_empleo_provincia.csv   \n",
       "33476  MERTRA           lavarropas_tasa_empleo_fem_provincia.csv   \n",
       "33477  MERTRA       tasa_empleo_mujer_salario_provincia_anio.csv   \n",
       "33485  MERTRA          tasa_empleo_provincia_nivel_educativo.csv   \n",
       "33517  MERTRA   tasa_empleo_por_franja_etaria_anio_provincia.csv   \n",
       "33521  MERTRA   tasa_empleo_por_franja_etaria_anio_provincia.csv   \n",
       "\n",
       "                                                problema  \n",
       "2      Para la string 'REP. DOMINICANA' se debería ge...  \n",
       "9      Para la string 'CABA' se debería generar un co...  \n",
       "25     Para la string 'TOTAL PAIS' se debería generar...  \n",
       "41     Para la string 'CIUDAD DE BUENOS AIRES' se deb...  \n",
       "55     Para la string 'NAN' se debería generar un cod...  \n",
       "2941   Para la string 'CIUDAD DE BUENOS AIRES' se deb...  \n",
       "2955   Para la string 'NAN' se debería generar un cod...  \n",
       "3461   Para la string 'TIGRES ASIATICOS' se debería g...  \n",
       "3726   Para la string 'CABA' se debería generar un co...  \n",
       "3753   Para la string 'GBA' se debería generar un cod...  \n",
       "5096   Para la string 'SAN_JUAN' se debería generar u...  \n",
       "5146   Para la string 'SANTA_CRUZ' se debería generar...  \n",
       "32782  Para la string 'CIUDAD DE BUENOS AIRES' se deb...  \n",
       "33363  Para la string 'NEA' se debería generar un cod...  \n",
       "33364  Para la string 'NOA' se debería generar un cod...  \n",
       "33395  Para la string 'CABA' se debería generar un co...  \n",
       "33398  Para la string 'TODAS' se debería generar un c...  \n",
       "33443  Para la string 'CIUDAD DE BUENOS AIRES' se deb...  \n",
       "33444  Para la string 'TOTAL' se debería generar un c...  \n",
       "33459  Para la string 'CABA' se debería generar un co...  \n",
       "33467  Para la string 'TOTAL PAIS' se debería generar...  \n",
       "33475  Para la string 'CABA' se debería generar un co...  \n",
       "33476  Para la string 'CABA' se debería generar un co...  \n",
       "33477  Para la string 'CIUDAD DE BUENOS AIRES' se deb...  \n",
       "33485  Para la string 'CIUDAD DE BUENOS AIRES' se deb...  \n",
       "33517  Para la string 'CABA' se debería generar un co...  \n",
       "33521  Para la string 'TOTAL' se debería generar un c...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tipo4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
